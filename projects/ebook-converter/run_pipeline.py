#!/usr/bin/env python3
import os
import sys
import logging
from pathlib import Path

from multi_cloud_downloader import MultiCloudDownloader
from pipeline_processor import PipelineProcessor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

import argparse

def main():
    parser = argparse.ArgumentParser(description='é›»å­æ›¸è½‰æ›ç®¡é“ - ç…‰é‡‘è¡“å£«ç‰ˆ')
    parser.add_argument('--path', type=str, default='/apps/bypy', help='ç™¾åº¦ç¶²ç›¤è·¯å¾‘')
    parser.add_argument('--workers', type=int, default=50, help='ä¸¦ç™¼ç·šç¨‹æ•¸')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶è™•ç†æ–‡ä»¶æ•¸é‡')
    parser.add_argument('--yes', '-y', action='store_true', help='è‡ªå‹•ç¢ºèªæ‰€æœ‰æç¤º')
    args = parser.parse_args()

    logger.info("=" * 70)
    logger.info("é›»å­æ›¸è½‰æ›ç®¡é“ - ç…‰é‡‘è¡“å£«ç‰ˆ (X.AI + MongoDB + S3)")
    logger.info("=" * 70)
    
    logger.info("\næ­¥é©Ÿ 1: åˆå§‹åŒ–çµ„ä»¶")
    # ç”¨æ–¼åˆ—å‡ºæ–‡ä»¶
    downloader = MultiCloudDownloader(cache_dir="/home/sms/ebook-converter/data/baidu-cache")
    
    # é«˜ä¸¦ç™¼è™•ç†å™¨
    processor = PipelineProcessor(
        cache_dir="/home/sms/ebook-converter/data/pipeline-cache",
        batch_size=args.workers,  # æ‰¹æ¬¡å¤§å°ç­‰æ–¼ä¸¦ç™¼æ•¸
        max_cache_gb=200
    )
    
    logger.info("\næ­¥é©Ÿ 2: é…ç½®é‹è¡Œåƒæ•¸")
    if not args.yes:
        remote_path = input(f"è«‹è¼¸å…¥ç™¾åº¦ç¶²ç›¤è·¯å¾‘ (é è¨­: {args.path}): ").strip() or args.path
        
        # é»˜èªå…¨é‡ï¼Œä½†å…è¨±ç”¨æˆ¶é™åˆ¶
        max_files_input = input(f"è¦è™•ç†å¤šå°‘å€‹æ–‡ä»¶ï¼Ÿ(é è¨­: å…¨éƒ¨ - æµå¼è™•ç†): ").strip()
        max_files = int(max_files_input) if max_files_input else args.limit
        
        # é»˜èªæ»¿è¼‰
        max_workers_input = input(f"ä¸¦ç™¼ç·šç¨‹æ•¸ï¼Ÿ(é è¨­: {args.workers} - æ»¿è¼‰æ¨¡å¼): ").strip()
        max_workers = int(max_workers_input) if max_workers_input else args.workers
    else:
        remote_path = args.path
        max_files = args.limit
        max_workers = args.workers
    
    logger.info(f"\næ­¥é©Ÿ 3: å•Ÿå‹•æµå¼è™•ç†")
    logger.info(f"- é ç¨‹è·¯å¾‘: {remote_path}")
    logger.info(f"- ä¸¦ç™¼ç·šç¨‹: {max_workers}")
    logger.info(f"- æ•¸é‡é™åˆ¶: {max_files if max_files else 'ç„¡é™åˆ¶'}")
    
    # ä½¿ç”¨ç”Ÿæˆå™¨ç²å–æ–‡ä»¶æµ
    file_generator = downloader.yield_baidu_files(remote_path)
    
    logger.info(f"\nğŸš€ å•Ÿå‹•æ»¿è¼‰æ¨¡å¼: {max_workers} ç·šç¨‹ä¸¦è¡Œè™•ç† (æµå¼è¼¸å…¥)...")
    
    # é‹è¡Œç®¡é“
    processor.run(files=file_generator, max_files=max_files, max_workers=max_workers)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\n\nç”¨æˆ¶ä¸­æ–·ï¼Œæ­£åœ¨ä¿å­˜é€²åº¦...")
    except Exception as e:
        logger.error(f"\n\nç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
