#!/usr/bin/env python3
"""
å¤ç±/ç¸£å¿—å°ˆç”¨è™•ç†ç®¡é“ (Ancient Book Pipeline)
å°ˆé–€è™•ç†è±æ’ã€æ¯›ç­†å­—ã€æƒæç‰ˆ PDF
ä½¿ç”¨ RapidOCR (Angle CLS Enabled) é€²è¡Œé«˜ç²¾åº¦è­˜åˆ¥
"""
import os
import json
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict

from multi_cloud_downloader import MultiCloudDownloader
from pdf_processor import PDFProcessor

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("/home/sms/ebook-converter/ancient_book_pipeline.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("AncientBookPipeline")

class AncientBookRunner:
    def __init__(self, 
                 workers=2, 
                 cache_dir="/home/sms/ebook-converter/data/ancient-book-cache",
                 output_dir="/home/sms/ebook-converter/data/markdown-output"):
        self.workers = workers
        self.cache_dir = Path(cache_dir)
        self.output_dir = Path(output_dir)
        
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.downloader = MultiCloudDownloader(cache_dir=str(self.cache_dir))
        # Processor instantiated per file to ensure thread safety with ONNX
        
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }

    def load_files(self) -> List[Dict]:
        json_path = Path("/home/sms/ebook-converter/data/ancient_books_list.json")
        if not json_path.exists():
            logger.error("æ‰¾ä¸åˆ°å¤ç±åˆ—è¡¨: ancient_books_list.json")
            return []
            
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        logger.info(f"åŠ è¼‰äº† {len(data)} æœ¬å¤ç±ä»»å‹™")
        return data

    def process_single_file(self, file_info):
        filename = file_info['name']
        relative_path = file_info.get('path', '').strip('/')
        
        # remove /çŸ¥è­˜åº« prefix if present
        safe_rel_path = relative_path
        if safe_rel_path.startswith("çŸ¥è­˜åº«/"):
            safe_rel_path = safe_rel_path[4:]
            
        target_dir = self.output_dir / safe_rel_path
        target_dir.mkdir(parents=True, exist_ok=True)
        
        target_file = target_dir / (Path(filename).stem + ".md")
        
        if target_file.exists():
            logger.info(f"â­ï¸ è·³éå·²å­˜åœ¨: {filename}")
            return 'skipped'

        logger.info(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰å¤ç±: {filename}")
        
        if 'source' not in file_info:
            file_info['source'] = 'baidu'
            
        local_path = self.downloader.download_file(file_info)
        
        if not local_path:
            logger.error(f"âŒ ä¸‹è¼‰å¤±æ•—: {filename}")
            return 'failed'
            
        try:
            # Instantiate processor (Ancient Mode is default now in PDFProcessor init)
            processor = PDFProcessor() 
            
            logger.info(f"ğŸ“œ æ­£åœ¨è­˜åˆ¥å¤ç± (OCR): {filename}")
            processor.extract_content(local_path, str(target_file))
            
            logger.info(f"âœ… è™•ç†å®Œæˆ: {filename}")
            return 'success'
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†å¤±æ•— {filename}: {e}")
            return 'failed'
            
        finally:
            try:
                if local_path and os.path.exists(local_path):
                    os.remove(local_path)
            except:
                pass

    def run(self):
        files = self.load_files()
        self.stats['total'] = len(files)
        
        logger.info(f"ğŸš€ å•Ÿå‹•å¤ç±å°ˆç”¨ç®¡é“ (Workers: {self.workers})")
        
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            future_to_file = {
                executor.submit(self.process_single_file, f): f 
                for f in files
            }
            
            for future in as_completed(future_to_file):
                file_info = future_to_file[future]
                try:
                    result = future.result()
                    self.stats[result] += 1
                except Exception as e:
                    logger.error(f"Unhandled exception for {file_info['name']}: {e}")
                    self.stats['failed'] += 1
                    
                processed = self.stats['success'] + self.stats['failed'] + self.stats['skipped']
                if processed % 5 == 0:
                    logger.info(f"ğŸ“Š å¤ç±é€²åº¦: {processed}/{self.stats['total']} (S:{self.stats['success']} F:{self.stats['failed']} Sk:{self.stats['skipped']})")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--workers', type=int, default=2, help='å¹¶å‘ OCR è¿›ç¨‹æ•°')
    args = parser.parse_args()
    
    runner = AncientBookRunner(workers=args.workers)
    runner.run()
