#!/usr/bin/env python3
"""
PDF è™•ç†ç®¡é“ - å°ˆé–€è™•ç† PDF æ–‡ä»¶ (åŒ…å« OCR)
ä¸‹è¼‰ â†’ æ™ºèƒ½è­˜åˆ¥(æ–‡å­—/æƒæ) â†’ OCR/æå– â†’ è½‰ Markdown â†’ ä¿å­˜
"""
import os
import json
import time
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict

from multi_cloud_downloader import MultiCloudDownloader
from pdf_processor import PDFProcessor

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("/home/sms/ebook-converter/pdf_pipeline.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("PDFPipeline")

class PDFPipelineRunner:
    def __init__(self, 
                 workers=2, 
                 cache_dir="/home/sms/ebook-converter/data/pdf-pipeline-cache",
                 output_dir="/home/sms/ebook-converter/data/markdown-output"):
        self.workers = workers
        self.cache_dir = Path(cache_dir)
        self.output_dir = Path(output_dir)
        
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.downloader = MultiCloudDownloader(cache_dir=str(self.cache_dir))
        self.processor = PDFProcessor() # Shared resource, but fitz/ocr methods are generally thread-safe or we instantiate per file?
        # RapidOCR might not be thread-safe if sharing same instance across threads. 
        # Better to instantiate inside the worker or use a lock.
        # For safety and simplicity, we'll instantiate inside process_single_file.
        
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'ocr_pages': 0
        }

    def load_files(self) -> List[Dict]:
        json_path = Path("/home/sms/ebook-converter/data/baidu-analysis/file_categories.json")
        if not json_path.exists():
            logger.error("æ‰¾ä¸åˆ°æ–‡ä»¶åˆ†é¡åˆ—è¡¨")
            return []
            
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # Combine all PDF types if needed, but report said they are in 'pdf_text' mostly
        pdfs = data.get('pdf_text', [])
        logger.info(f"åŠ è¼‰äº† {len(pdfs)} å€‹ PDF ä»»å‹™")
        return pdfs

    def process_single_file(self, file_info):
        filename = file_info['name']
        relative_path = file_info.get('path', '').strip('/')
        
        # 1. Check if output exists
        # Construct output path preserving directory structure
        # /çŸ¥è­˜åº«/A/B/file.pdf -> data/markdown-output/çŸ¥è­˜åº«/A/B/file.md
        
        # remove /çŸ¥è­˜åº« prefix if present to avoid double root
        safe_rel_path = relative_path
        if safe_rel_path.startswith("çŸ¥è­˜åº«/"):
            safe_rel_path = safe_rel_path[4:]
            
        target_dir = self.output_dir / safe_rel_path
        target_dir.mkdir(parents=True, exist_ok=True)
        
        target_file = target_dir / (Path(filename).stem + ".md")
        
        if target_file.exists():
            logger.info(f"â­ï¸ è·³éå·²å­˜åœ¨: {filename}")
            return 'skipped'

        logger.info(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰: {filename}")
        
        # 2. Download
        if 'source' not in file_info:
            file_info['source'] = 'baidu'
            
        local_path = self.downloader.download_file(file_info)
        
        if not local_path:
            logger.error(f"âŒ ä¸‹è¼‰å¤±æ•—: {filename}")
            return 'failed'
            
        try:
            # 3. Process
            # Instantiate processor here to avoid thread safety issues with ONNX runtime
            processor = PDFProcessor() 
            
            logger.info(f"âš™ï¸ æ­£åœ¨è™•ç† (OCR): {filename}")
            processor.extract_content(local_path, str(target_file))
            
            logger.info(f"âœ… è™•ç†å®Œæˆ: {filename}")
            return 'success'
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†å¤±æ•— {filename}: {e}")
            return 'failed'
            
        finally:
            # 4. Cleanup
            try:
                if local_path and os.path.exists(local_path):
                    os.remove(local_path)
            except:
                pass

    def run(self):
        files = self.load_files()
        self.stats['total'] = len(files)
        
        logger.info(f"ğŸš€ å•Ÿå‹• PDF è™•ç†ç®¡é“ (Workers: {self.workers})")
        
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            # Map futures
            future_to_file = {
                executor.submit(self.process_single_file, f): f 
                for f in files
            }
            
            for future in as_completed(future_to_file):
                file_info = future_to_file[future]
                try:
                    result = future.result()
                    self.stats[result] += 1
                except Exception as e:
                    logger.error(f"Unhandled exception for {file_info['name']}: {e}")
                    self.stats['failed'] += 1
                    
                # Log progress
                processed = self.stats['success'] + self.stats['failed'] + self.stats['skipped']
                if processed % 10 == 0:
                    logger.info(f"ğŸ“Š é€²åº¦: {processed}/{self.stats['total']} (S:{self.stats['success']} F:{self.stats['failed']} Sk:{self.stats['skipped']})")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--workers', type=int, default=2, help='å¹¶å‘ OCR è¿›ç¨‹æ•° (å»ºè®® 2-4)')
    args = parser.parse_args()
    
    runner = PDFPipelineRunner(workers=args.workers)
    runner.run()
