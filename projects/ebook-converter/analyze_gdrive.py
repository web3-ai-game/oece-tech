import os
from pathlib import Path
import json
import logging
from collections import defaultdict
import datetime

logging.basicConfig(level=logging.INFO, format='%(message)s')

def analyze_gdrive():
    print("ğŸ” å•Ÿå‹• Google Drive æ–‡ä»¶æ·±åº¦å¯©è¨ˆ...")
    
    gdrive_path = Path("/home/sms/mnt/gdrive")
    report_file = Path("/home/sms/ebook-converter/data/gdrive_analysis.json")
    
    if not gdrive_path.exists():
        print("âŒ Google Drive æ›è¼‰é»ä¸å­˜åœ¨")
        return

    stats = {
        "total_files": 0,
        "total_size_mb": 0,
        "by_type": defaultdict(int),
        "personal_docs": [],
        "media_files": [],
        "code_files": [],
        "long_filenames": [], # Potential voice-to-text or messy names
        "recent_files": []
    }
    
    # Keywords indicating personal content
    personal_keywords = ["ç®€å†", "resume", "èº«ä»½è¯", "photo", "ç…§ç‰‡", "backup", "å¤‡ä»½", "å¯†ç ", "password", "private", "ç§", "æˆ‘çš„", "me", "diary", "æ—¥è®°"]
    
    print(f"æ­£åœ¨æƒæ: {gdrive_path}")
    
    try:
        for root, dirs, files in os.walk(gdrive_path):
            for file in files:
                file_path = Path(root) / file
                stats["total_files"] += 1
                
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    stats["total_size_mb"] += size_mb
                    mtime = datetime.datetime.fromtimestamp(file_path.stat().st_mtime)
                except:
                    size_mb = 0
                    mtime = datetime.datetime.min
                
                ext = file_path.suffix.lower()
                stats["by_type"][ext] += 1
                
                # Check for messy/long filenames (often from VTT or casual saves)
                if len(file) > 50:
                    stats["long_filenames"].append({
                        "name": file,
                        "path": str(file_path),
                        "length": len(file)
                    })
                
                # Categorize
                is_personal = False
                for kw in personal_keywords:
                    if kw in file.lower():
                        stats["personal_docs"].append(str(file_path))
                        is_personal = True
                        break
                
                if not is_personal:
                    if ext in ['.jpg', '.jpeg', '.png', '.heic', '.mp4', '.mov']:
                        stats["media_files"].append(str(file_path))
                    elif ext in ['.py', '.js', '.html', '.css', '.json', '.sh']:
                        stats["code_files"].append(str(file_path))
                        
                # Recent files (last 30 days)
                if (datetime.datetime.now() - mtime).days < 30:
                    stats["recent_files"].append(str(file_path))
                    
                if stats["total_files"] % 100 == 0:
                    print(f"å·²æƒæ {stats['total_files']} å€‹æ–‡ä»¶...", end='\r')
                    
    except Exception as e:
        print(f"æƒæéç¨‹ä¸­å‡ºéŒ¯: {e}")

    print(f"\nâœ… æƒæå®Œæˆã€‚ç¸½è¨ˆç™¼ç¾ {stats['total_files']} å€‹æ–‡ä»¶ï¼Œå…± {stats['total_size_mb']:.2f} MB")
    
    # Save raw data
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
        
    # Print Summary Report
    print("\n" + "="*50)
    print("ğŸ“Š Google Drive åˆ†é¡å¯©è¨ˆå ±å‘Š")
    print("="*50)
    
    print(f"ğŸ“‚ æ–‡ä»¶é¡å‹åˆ†å¸ƒ (Top 10):")
    sorted_types = sorted(stats["by_type"].items(), key=lambda x: x[1], reverse=True)
    for ext, count in sorted_types[:10]:
        print(f"  - {ext or 'No Ext'}: {count}")
        
    print(f"\nğŸ”’ ç–‘ä¼¼å€‹äººæ•æ„Ÿæ–‡ä»¶ ({len(stats['personal_docs'])}):")
    for f in stats["personal_docs"][:5]:
        print(f"  - {Path(f).name}")
    if len(stats['personal_docs']) > 5:
        print(f"  ...ç­‰å…± {len(stats['personal_docs'])} å€‹")
        
    print(f"\nğŸ“¸ åª’é«”/ç…§ç‰‡æ–‡ä»¶ ({len(stats['media_files'])}):")
    print(f"  (å»ºè­°æ­¸æª”åˆ° Photos æˆ–å–®ç¨å­˜å„²)")
    
    print(f"\nğŸ—£ï¸ é•·æ–‡ä»¶å/èªéŸ³ç­†è¨˜ ({len(stats['long_filenames'])}):")
    for item in stats["long_filenames"][:5]:
        print(f"  - {item['name'][:40]}...")
        
    print("\nğŸ’¡ å»ºè­°:")
    if stats['media_files']:
        print("- ç™¼ç¾å¤§é‡åª’é«”æ–‡ä»¶ï¼Œå»ºè­°ä½¿ç”¨ PhotoPrism æˆ– Google Photos æ•´ç†ã€‚")
    if stats['long_filenames']:
        print("- ç™¼ç¾è¨±å¤šé•·æ–‡ä»¶åï¼ˆå¯èƒ½ä¾†è‡ªèªéŸ³è½‰æ–‡å­—ï¼‰ï¼Œå»ºè­°ä½¿ç”¨ AI é‡å‘½åæ•´ç†ã€‚")
    print("- å€‹äººæ–‡ä»¶å»ºè­°å–®ç¨åŠ å¯†æˆ–ç§»è‡³ Private æ–‡ä»¶å¤¾ã€‚")

if __name__ == "__main__":
    analyze_gdrive()
