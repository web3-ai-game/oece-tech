#!/usr/bin/env python3
"""
æµå¼è™•ç†ç®¡é“ - ç‚ºæ¨æ¼”ç¨®å­åº«è¨­è¨ˆ
ä¸‹è¼‰ â†’ æ¸…æ´— â†’ è½‰æ› â†’ ä¸Šå‚³ S3 â†’ å¯«å…¥ MongoDB â†’ æ¸…ç†æœ¬åœ°
"""
import os
import json
import subprocess
import hashlib
import time
import random
from pathlib import Path
from typing import List, Dict, Optional, Iterable
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

from dedup_processor import DedupProcessor
from main import EbookConverterPipeline
from multi_cloud_downloader import MultiCloudDownloader

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PipelineProcessor:
    """æµå¼è™•ç†ç®¡é“ - ç…‰é‡‘è¡“å£«æ¨¡å‹"""
    
    def __init__(self, 
                 cache_dir="/home/sms/ebook-converter/data/pipeline-cache",
                 batch_size=50,
                 max_cache_gb=80):
        """
        åˆå§‹åŒ–è™•ç†ç®¡é“
        
        Args:
            cache_dir: æœ¬åœ°ç·©å­˜ç›®éŒ„ï¼ˆSSDï¼‰
            batch_size: æ¯æ‰¹è™•ç†æ–‡ä»¶æ•¸
            max_cache_gb: æœ€å¤§ç·©å­˜å¤§å°ï¼ˆGBï¼‰ï¼Œé˜²æ­¢æ’çˆ† SSD
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.batch_size = batch_size
        self.max_cache_bytes = max_cache_gb * 1024 * 1024 * 1024
        
        # åŠ è¼‰åˆ†æçµæœ
        self.analysis_dir = Path("/home/sms/ebook-converter/data/baidu-analysis")
        self.categories_file = self.analysis_dir / "file_categories.json"
        
        # è™•ç†å™¨
        self.dedup = DedupProcessor()
        self.converter = EbookConverterPipeline()
        self.downloader = MultiCloudDownloader(cache_dir=str(self.cache_dir))
        
        # MongoDB é€£æ¥ï¼ˆå¯é¸ï¼‰
        self.mongo_enabled = True
        self.mongo_backup = Path("/home/sms/ebook-converter/data/mongodb-backup.jsonl")
        
        # S3 ä¸Šå‚³ï¼ˆå¯é¸ï¼‰
        self.s3_enabled = True
        
        # çµ±è¨ˆ
        self.stats = {
            'total': 0,
            'processed': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'uploaded_s3': 0,
            'saved_mongo': 0
        }
    
    def load_txt_files(self) -> List[Dict]:
        """åŠ è¼‰ TXT æ–‡ä»¶åˆ—è¡¨"""
        logger.info("ğŸ“‚ åŠ è¼‰ TXT æ–‡ä»¶åˆ—è¡¨...")
        
        with open(self.categories_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        
        txt_files = categories.get('txt_files', [])
        logger.info(f"âœ“ æ‰¾åˆ° {len(txt_files)} å€‹ TXT æ–‡ä»¶")
        
        return txt_files
    
    def read_file_safe(self, file_path: str) -> str:
        """æ™ºèƒ½è®€å–æ–‡ä»¶ï¼Œè‡ªå‹•è™•ç† GBK/GB18030/UTF-8 ç·¨ç¢¼å•é¡Œ"""
        # 1. ä½¿ç”¨ chardet æª¢æ¸¬ç·¨ç¢¼
        detected_encoding = None
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(50000)  # è®€å–å‰ 50KB é€²è¡Œæª¢æ¸¬
                result = chardet.detect(raw_data)
                if result['confidence'] > 0.8:
                    detected_encoding = result['encoding']
        except Exception as e:
            logger.warning(f"Chardet æª¢æ¸¬å¤±æ•—: {e}")

        # æ§‹å»ºå˜—è©¦åˆ—è¡¨
        encodings = []
        if detected_encoding:
            encodings.append(detected_encoding)
        
        # å¸¸ç”¨ä¸­æ–‡ç·¨ç¢¼è£œå……
        encodings.extend(['utf-8', 'gb18030', 'gbk', 'gb2312', 'big5', 'utf-16', 'latin1'])
        
        # å»é‡ä¿æŒé †åº
        seen = set()
        unique_encodings = []
        for enc in encodings:
            if not enc: continue
            # çµ±ä¸€è½‰å°å¯«æ¯”è¼ƒ
            enc_lower = enc.lower()
            if enc_lower == 'gb2312': enc_lower = 'gb18030' # GB18030 å…¼å®¹ GB2312ï¼Œå„ªå…ˆä½¿ç”¨
            
            if enc_lower not in seen:
                unique_encodings.append(enc)
                seen.add(enc_lower)

        for enc in unique_encodings:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    return f.read()
            except (UnicodeDecodeError, LookupError):
                continue
        
        # æœ€å¾Œå˜—è©¦ï¼šå¿½ç•¥éŒ¯èª¤
        logger.warning(f"âš ï¸ ç„¡æ³•è­˜åˆ¥ç·¨ç¢¼ï¼Œä½¿ç”¨ UTF-8 å¿½ç•¥éŒ¯èª¤: {file_path}")
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    
    def calculate_file_hash(self, file_path: str) -> str:
        """è¨ˆç®—æ–‡ä»¶ hashï¼ˆç”¨ä½œå”¯ä¸€ IDï¼‰"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except:
            return hashlib.md5(file_path.encode()).hexdigest()
    
    def get_cache_size(self) -> int:
        """ç²å–ç•¶å‰ç·©å­˜å¤§å°"""
        total_size = 0
        for file in self.cache_dir.rglob('*'):
            if file.is_file():
                total_size += file.stat().st_size
        return total_size
    
    def clean_cache(self):
        """æ¸…ç†ç·©å­˜ç›®éŒ„"""
        logger.info("ğŸ§¹ æ¸…ç†æœ¬åœ°ç·©å­˜...")
        for file in self.cache_dir.rglob('*'):
            if file.is_file():
                try:
                    file.unlink()
                except Exception as e:
                    logger.warning(f"ç„¡æ³•åˆªé™¤ {file}: {e}")
        logger.info("âœ“ ç·©å­˜å·²æ¸…ç†")
    
    def download_file(self, file_info: Dict) -> Optional[str]:
        """
        å¾é›²ç›¤ä¸‹è¼‰æ–‡ä»¶åˆ°æœ¬åœ°ç·©å­˜
        
        Returns:
            æœ¬åœ°æ–‡ä»¶è·¯å¾‘ï¼Œå¤±æ•—è¿”å› None
        """
        # æª¢æŸ¥ç·©å­˜å¤§å°
        if self.get_cache_size() > self.max_cache_bytes:
            logger.warning(f"âš ï¸ ç·©å­˜å·²æ»¿ï¼Œæ¸…ç†ä¸­...")
            self.clean_cache()
            
        # ç¢ºä¿æœ‰ source å­—æ®µ (å…¼å®¹èˆŠæ•¸æ“š)
        if 'source' not in file_info:
            file_info['source'] = 'baidu'
            
        # ä½¿ç”¨çµ±ä¸€ä¸‹è¼‰å™¨
        return self.downloader.download_file(file_info)
    
    def process_single_file_safe(self, file_info: Dict) -> Dict:
        """
        å®‰å…¨è™•ç†å–®å€‹æ–‡ä»¶ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰
        
        Returns:
            è™•ç†çµæœå­—å…¸
        """
        filename = file_info['name']
        max_retries = 3
        base_wait = 2
        
        result = {
            'filename': filename,
            'status': 'pending',
            'local_path': None,
            'markdown_path': None,
            'doc_id': None,
            'mongo_id': None,
            'error': None
        }
        
        for attempt in range(max_retries):
            try:
                # 1. ä¸‹è¼‰æ–‡ä»¶
                local_path = self.download_file(file_info)
                if not local_path:
                    result['status'] = 'download_failed'
                    result['error'] = 'ä¸‹è¼‰å¤±æ•—'
                    return result
                
                result['local_path'] = local_path
                
                # 2. èª¿ç”¨è½‰æ›ç®¡é“ (åŒ…å« è½‰æ› -> ç´¢å¼• -> S3 -> Mongo)
                logger.info(f"ğŸ”„ è™•ç†: {filename}")
                pipeline_result = self.converter.process_single_file(local_path)
                
                # 3. è™•ç†çµæœ
                if pipeline_result.get('skipped'):
                    result['status'] = 'skipped'
                    self.stats['skipped'] += 1
                    logger.info(f"â­ï¸ è·³é: {filename} ({pipeline_result.get('reason')})")
                    # æ¸…ç†æœ¬åœ°æ–‡ä»¶
                    try:
                         if os.path.exists(local_path):
                            os.remove(local_path)
                    except:
                        pass
                    return result

                if not pipeline_result.get('success'):
                    raise Exception(pipeline_result.get('error', 'æœªçŸ¥è½‰æ›éŒ¯èª¤'))
                
                # æˆåŠŸ
                result['status'] = 'success'
                result['markdown_path'] = pipeline_result.get('markdown_path')
                result['doc_id'] = pipeline_result.get('doc_id')
                result['mongo_id'] = pipeline_result.get('mongo_id')
                
                self.stats['success'] += 1
                self.stats['uploaded_s3'] += 1 # main.py handles this
                if result['mongo_id']:
                    self.stats['saved_mongo'] += 1

                # 4. å‚™ä»½æ—¥èªŒ
                self.backup_to_jsonl(file_info, result)
                
                # 5. æ¸…ç†æœ¬åœ°æ–‡ä»¶
                try:
                    if os.path.exists(local_path):
                        os.remove(local_path)
                except:
                    pass
                
                logger.info(f"âœ… å®Œæˆ: {filename}")
                return result
                
            except Exception as e:
                error_msg = str(e)
                
                # API éŒ¯èª¤é‡è©¦
                if "500" in error_msg or "429" in error_msg or "Internal" in error_msg:
                    wait_time = base_wait * (attempt + 1) + random.uniform(0, 1)
                    logger.warning(f"ğŸ”„ API éŒ¯èª¤ï¼Œç­‰å¾… {wait_time:.1f}s é‡è©¦ ({attempt+1}/{max_retries})...")
                    time.sleep(wait_time)
                else:
                    result['status'] = 'failed'
                    result['error'] = error_msg
                    self.stats['failed'] += 1
                    logger.error(f"âŒ å¤±æ•—: {filename} - {error_msg}")
                    return result
        
        result['status'] = 'failed'
        result['error'] = f'é‡è©¦ {max_retries} æ¬¡å¤±æ•—'
        self.stats['failed'] += 1
        logger.error(f"ğŸ’€ æ”¾æ£„: {filename}")
        return result
    
    def backup_to_jsonl(self, file_info: Dict, result: Dict):
        """å‚™ä»½åˆ° JSONL æ–‡ä»¶"""
        record = {
            'filename': file_info['name'],
            'original_path': file_info['full_path'],
            'size': file_info['size'],
            'doc_id': result.get('doc_id'),
            'mongo_id': result.get('mongo_id'),
            'status': result['status'],
            'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(self.mongo_backup, 'a', encoding='utf-8') as f:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')
    
    def process_batch(self, files: List[Dict], max_workers: int = 8):
        """
        æ‰¹é‡è™•ç†æ–‡ä»¶ï¼ˆä¸¦ç™¼ï¼‰
        
        Args:
            files: æ–‡ä»¶åˆ—è¡¨
            max_workers: æœ€å¤§ä¸¦ç™¼æ•¸
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ”¥ é–‹å§‹æ‰¹é‡è™•ç†: {len(files)} å€‹æ–‡ä»¶")
        logger.info(f"ä¸¦ç™¼æ•¸: {max_workers}")
        logger.info(f"{'='*80}\n")
        
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(self.process_single_file_safe, f): f['name']
                for f in files
            }
            
            for i, future in enumerate(as_completed(future_to_file), 1):
                filename = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    # é€²åº¦é¡¯ç¤º
                    elapsed = time.time() - start_time
                    avg_time = elapsed / i
                    remaining = (len(files) - i) * avg_time
                    
                    logger.info(f"[{i}/{len(files)}] {result['status']} | "
                              f"é è¨ˆå‰©é¤˜: {remaining/60:.1f}åˆ†")
                    
                except Exception as exc:
                    logger.error(f"ğŸ’¥ ç·šç¨‹å´©æ½°: {filename} - {exc}")
        
        elapsed = time.time() - start_time
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“Š æ‰¹æ¬¡è™•ç†å®Œæˆï¼è€—æ™‚: {elapsed/60:.1f} åˆ†é˜")
        logger.info(f"{'='*80}\n")
        
        return results

    def run(self, files: Iterable[Dict] = None, max_files: Optional[int] = None, max_workers: int = 50):
        """
        é‹è¡Œå®Œæ•´è™•ç†æµç¨‹ (æ”¯æ´æµå¼è¼¸å…¥)
        
        Args:
            files: è¦è™•ç†çš„æ–‡ä»¶è¿­ä»£å™¨ (List or Generator)
            max_files: æœ€å¤§è™•ç†æ–‡ä»¶æ•¸ï¼ˆNone = å…¨éƒ¨ï¼‰
            max_workers: ä¸¦ç™¼æ•¸
        """
        logger.info("="*80)
        logger.info("ğŸš€ æµå¼è™•ç†ç®¡é“å•Ÿå‹• - ç…‰é‡‘è¡“å£«ç‰ˆ (X.AI + MongoDB) [Streaming Mode]")
        logger.info("="*80)
        
        if files is None:
            # Fallback to local list
            try:
                files = self.load_txt_files()
            except Exception as e:
                logger.warning(f"ç„¡æ³•åŠ è¼‰æœ¬åœ°æ–‡ä»¶åˆ—è¡¨: {e}")
                files = []
            
        # Stream processing with batching
        batch = []
        processed_count = 0
        skipped_count = 0
        
        logger.info("\næ­¥é©Ÿ 1: é–‹å§‹æµå¼è™•ç† (å³æ™‚å»é‡ -> åˆ†æ‰¹ -> åŸ·è¡Œ)...")
        
        try:
            for file_info in files:
                # 1. Dedup on the fly
                if self.dedup.is_processed(file_info):
                    skipped_count += 1
                    if skipped_count % 100 == 0:
                        logger.info(f"å·²è·³é {skipped_count} å€‹é‡è¤‡æ–‡ä»¶...")
                    continue
                
                batch.append(file_info)
                
                # 2. Check batch size
                if len(batch) >= self.batch_size:
                    batch_num = processed_count // self.batch_size + 1
                    logger.info(f"\n{'='*80}")
                    logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num}: è™•ç† {len(batch)} å€‹æ–‡ä»¶")
                    logger.info(f"{'='*80}")
                    
                    self.process_batch(batch, max_workers)
                    processed_count += len(batch)
                    batch = []
                    
                    # Clean cache
                    if self.get_cache_size() > self.max_cache_bytes * 0.8:
                        self.clean_cache()
                
                # 3. Check max limit
                if max_files and processed_count >= max_files:
                    logger.info(f"é”åˆ°æœ€å¤§è™•ç†æ•¸é‡é™åˆ¶: {max_files}")
                    break
            
            # Process remaining batch
            if batch and (not max_files or processed_count < max_files):
                batch_num = processed_count // self.batch_size + 1
                logger.info(f"\n{'='*80}")
                logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num} (å°¾éƒ¨): è™•ç† {len(batch)} å€‹æ–‡ä»¶")
                logger.info(f"{'='*80}")
                self.process_batch(batch, max_workers)
                processed_count += len(batch)
                
        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ ç”¨æˆ¶ä¸­æ–·è™•ç† stream")
            # Don't raise, let summary print
        except Exception as e:
            logger.error(f"Stream è™•ç†ç•°å¸¸: {e}", exc_info=True)
            
        self.stats['total'] = processed_count + skipped_count 
        self.stats['skipped'] = skipped_count
        
        # 5. æœ€çµ‚çµ±è¨ˆ
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°è™•ç†ç¸½çµ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š è™•ç†ç¸½çµ")
        logger.info("="*80)
        logger.info(f"ç¸½æ–‡ä»¶æ•¸: {self.stats['total']}")
        logger.info(f"âœ… æˆåŠŸ: {self.stats['success']}")
        logger.info(f"âŒ å¤±æ•—: {self.stats['failed']}")
        logger.info(f"â­ï¸ è·³é: {self.stats['skipped']}")
        
        if self.s3_enabled:
            logger.info(f"â˜ï¸ ä¸Šå‚³ S3: {self.stats['uploaded_s3']}")
        
        if self.mongo_enabled:
            logger.info(f"ğŸ—„ï¸ å¯«å…¥ MongoDB: {self.stats['saved_mongo']}")
        else:
            logger.info(f"ğŸ’¾ å‚™ä»½åˆ°: {self.mongo_backup}")
        
        logger.info(f"ğŸ“ Markdown è¼¸å‡º: /home/sms/ebook-converter/data/markdown-output/")
        logger.info("="*80)


def main():
    """ä¸»ç¨‹åº"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æµå¼è™•ç†ç®¡é“ - ç…‰é‡‘è¡“å£«æ¨¡å‹')
    parser.add_argument('-n', '--num', type=int, default=None, 
                       help='è™•ç†æ–‡ä»¶æ•¸é‡ï¼ˆé»˜èªå…¨éƒ¨ï¼‰')
    parser.add_argument('-w', '--workers', type=int, default=100,
                       help='ä¸¦ç™¼æ•¸ï¼ˆé»˜èª 100 - æ¥µé™ç‹‚æš´æ¨¡å¼ï¼‰')
    parser.add_argument('-b', '--batch', type=int, default=100,
                       help='æ¯æ‰¹è™•ç†æ•¸é‡ï¼ˆé»˜èª 100ï¼‰')
    parser.add_argument('--cache-gb', type=int, default=80,
                       help='æœ€å¤§ç·©å­˜å¤§å° GBï¼ˆé»˜èª 80ï¼‰')
    
    args = parser.parse_args()
    
    processor = PipelineProcessor(
        batch_size=args.batch,
        max_cache_gb=args.cache_gb
    )
    
    processor.run(max_files=args.num, max_workers=args.workers)


if __name__ == "__main__":
    main()
