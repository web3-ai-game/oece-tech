#!/usr/bin/env python3
"""çµ±ä¸€çš„é›»å­æ›¸è™•ç†ç®¡é“ - æ”¯æ´å¤šé›²ç›¤"""
import os
import sys
from pathlib import Path
from multi_cloud_downloader import MultiCloudDownloader
from main import EbookConverterPipeline
from fast_track_runner import FastTrackRunner
from config import MD_OUTPUT_DIR
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class UnifiedEbookPipeline:
    """çµ±ä¸€çš„é›»å­æ›¸è™•ç†ç®¡é“"""
    
    def __init__(self):
        self.downloader = MultiCloudDownloader()
        self.converter = EbookConverterPipeline()
        self.fast_track = FastTrackRunner()
        self.max_disk_usage_gb = 200  # æœ€å¤§ç£ç›¤ä½¿ç”¨é‡
    
    def check_disk_space(self) -> float:
        """æª¢æŸ¥å¯ç”¨ç£ç›¤ç©ºé–“ï¼ˆGBï¼‰"""
        stat = os.statvfs('/home/sms')
        free_gb = (stat.f_bavail * stat.f_frsize) / (1024**3)
        return free_gb
    
    def process_cloud_ebooks(self, max_files: int = None, sources: list = None):
        """è™•ç†é›²ç›¤é›»å­æ›¸ (Stream processing)"""
        logger.info("=" * 60)
        logger.info("çµ±ä¸€é›»å­æ›¸è™•ç†ç®¡é“")
        logger.info("=" * 60)
        
        # æª¢æŸ¥ç£ç›¤ç©ºé–“
        free_space = self.check_disk_space()
        logger.info(f"å¯ç”¨ç£ç›¤ç©ºé–“: {free_space:.1f} GB")
        
        if free_space < 10:
            logger.error("ç£ç›¤ç©ºé–“ä¸è¶³ 10GBï¼Œè«‹æ¸…ç†å¾Œå†è©¦")
            return
        
        # ä½¿ç”¨ç”Ÿæˆå™¨æƒæ
        logger.info("\né–‹å§‹æƒæä¸¦è™•ç†é›²ç›¤æ–‡ä»¶ (æµå¼è™•ç†)...")
        ebook_generator = self.downloader.yield_all_ebooks()
        
        # è™•ç†æ¯å€‹æ–‡ä»¶
        processed = 0
        failed = 0
        skipped = 0
        scanned_count = 0
        
        for file_info in ebook_generator:
            scanned_count += 1
            
            # éæ¿¾ä¾†æº
            if sources and file_info['source'] not in sources:
                continue
                
            # é™åˆ¶æ•¸é‡ (Check before processing)
            if max_files and processed >= max_files:
                logger.info(f"\nå·²é”åˆ°è™•ç†æ•¸é‡é™åˆ¶ ({max_files})ï¼Œåœæ­¢è™•ç†ã€‚")
                break
            
            logger.info(f"\n{'='*60}")
            logger.info(f"æƒæåˆ°ç¬¬ {scanned_count} å€‹æ–‡ä»¶: {file_info['name']}")
            logger.info(f"ä¾†æº: {file_info['source'].upper()}")
            logger.info(f"{'='*60}")
            
            # 0. æª¢æŸ¥æ˜¯å¦ç‚ºå¤ç±/ç¸£å¿— (Fast Track)
            is_ancient = any(k in file_info.get('path', '') for k in ['å¤ç±', 'å¿å¿—', 'æ–¹å¿—', 'å››åº“', 'å¤§è—ç»', 'åœ°æ–¹å¿—'])
            if is_ancient:
                logger.info("ğŸ“œ æª¢æ¸¬åˆ°å¤ç±/ç¸£å¿—ï¼Œä½¿ç”¨ Fast Track é€šé“ (è·³é OCR/è½‰æ›)...")
                try:
                    result = self.fast_track.process_single_file(file_info)
                    if result == 'success':
                        processed += 1
                        logger.info(f"âœ“ å¤ç±æ­¸æª”æˆåŠŸ: {file_info['name']}")
                    elif result == 'skipped':
                        skipped += 1
                        logger.info(f"â­ï¸ è·³é: {file_info['name']}")
                    else:
                        failed += 1
                        logger.error(f"å¤ç±è™•ç†å¤±æ•—: {file_info['name']}")
                except Exception as e:
                    logger.error(f"Fast Track åŸ·è¡Œå¤±æ•—: {e}")
                    failed += 1
                
                # Fast Track handles its own cleanup
                continue

            try:
                # 1. ä¸‹è¼‰æ–‡ä»¶
                logger.info("æ­¥é©Ÿ 1/4: ä¸‹è¼‰æ–‡ä»¶...")
                local_path = self.downloader.download_file(file_info)
                
                if not local_path or not os.path.exists(local_path):
                    logger.error("ä¸‹è¼‰å¤±æ•—ï¼Œè·³é")
                    failed += 1
                    continue
                
                # 2. è½‰æ›ç‚º Markdown
                logger.info("æ­¥é©Ÿ 2/4: è½‰æ›ç‚º Markdown...")
                success = self.converter.process_single_file(local_path)
                
                # Handle dictionary response from process_single_file
                if isinstance(success, dict):
                    if success.get('success'):
                        if success.get('skipped'):
                            logger.info(f"â­ï¸ è·³é (å·²å­˜åœ¨): {file_info['name']}")
                            skipped += 1
                        else:
                            logger.info(f"âœ“ æˆåŠŸè™•ç†: {file_info['name']}")
                            processed += 1
                    else:
                        logger.error(f"è½‰æ›å¤±æ•—: {success.get('error')}")
                        failed += 1
                elif success: # Boolean fallback
                    processed += 1
                    logger.info(f"âœ“ æˆåŠŸè™•ç†: {file_info['name']}")
                else:
                    logger.error("è½‰æ›å¤±æ•—")
                    failed += 1
                
                # 3. æ¸…ç†æœ¬åœ°æ–‡ä»¶ï¼ˆç¯€çœç©ºé–“ï¼‰
                logger.info("æ­¥é©Ÿ 3/4: æ¸…ç†ç·©å­˜...")
                try:
                    if local_path and os.path.exists(local_path):
                        os.remove(local_path)
                        logger.info(f"å·²åˆªé™¤ç·©å­˜: {local_path}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†å¤±æ•—: {e}")
                
                # 4. æª¢æŸ¥ç£ç›¤ç©ºé–“
                free_space = self.check_disk_space()
                
                if free_space < 5:
                    logger.warning(f"ç£ç›¤ç©ºé–“ä¸è¶³ ({free_space:.1f} GB)ï¼ŒåŸ·è¡Œé¡å¤–æ¸…ç†...")
                    self.downloader.cleanup_cache(keep_files=0)
                
            except Exception as e:
                logger.error(f"è™•ç†å¤±æ•—: {e}")
                failed += 1
                continue
        
        # ç¸½çµ
        logger.info("\n" + "=" * 60)
        logger.info("è™•ç†å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"æƒæç¸½æ•¸: {scanned_count}")
        logger.info(f"è™•ç†æˆåŠŸ: {processed}")
        logger.info(f"å·²è·³é: {skipped}")
        logger.info(f"å¤±æ•—: {failed}")
        logger.info("=" * 60)
        
        # 5. ç”Ÿæˆç¸½ç›®éŒ„
        logger.info("æ­¥é©Ÿ 5/5: æ›´æ–°ç¸½ç›®éŒ„ (CATALOG.md)...")
        try:
            from generate_master_catalog import CatalogGenerator
            generator = CatalogGenerator(MD_OUTPUT_DIR)
            generator.generate()
            logger.info("ç›®éŒ„æ›´æ–°å®Œæˆ")
        except Exception as e:
            logger.error(f"ç›®éŒ„ç”Ÿæˆå¤±æ•—: {e}")

        # æŸ¥çœ‹è¼¸å‡º
        logger.info("\nMarkdown è¼¸å‡ºç›®éŒ„:")
        logger.info(f"  {MD_OUTPUT_DIR}")
        logger.info("\nç¶­æ ¹æ–¯å¦ç´¢å¼•:")
        logger.info("  /home/sms/ebook-converter/data/wittgenstein-index/")


def main():
    """ä¸»ç¨‹åº"""
    import argparse
    
    parser = argparse.ArgumentParser(description="çµ±ä¸€é›»å­æ›¸è™•ç†ç®¡é“")
    parser.add_argument("--auto", action="store_true", help="è™•ç†æ‰€æœ‰é›²ç›¤çš„é›»å­æ›¸ï¼ˆè‡ªå‹•æ¨¡å¼ï¼‰")
    parser.add_argument("--baidu", action="store_true", help="åªè™•ç†ç™¾åº¦ç¶²ç›¤")
    parser.add_argument("--gdrive", action="store_true", help="åªè™•ç† Google Drive")
    parser.add_argument("--limit", type=int, help="è™•ç†æŒ‡å®šæ•¸é‡çš„æ–‡ä»¶")
    
    args = parser.parse_args()
    
    pipeline = UnifiedEbookPipeline()
    
    # CLI Mode
    if args.auto or args.baidu or args.gdrive or args.limit:
        sources = []
        if args.baidu: sources.append('baidu')
        if args.gdrive: sources.append('gdrive')
        
        # Default to all if not specified but auto/limit is present
        if not sources and (args.auto or args.limit):
            sources = None 
            
        pipeline.process_cloud_ebooks(max_files=args.limit, sources=sources)
        return

    # Interactive Mode
    print("\n" + "=" * 60)
    print("çµ±ä¸€é›»å­æ›¸è™•ç†ç®¡é“")
    print("æ”¯æ´ï¼šç™¾åº¦ç¶²ç›¤ã€Google Drive")
    print("=" * 60)
    
    print("\né¸é …ï¼š")
    print("1. è™•ç†æ‰€æœ‰é›²ç›¤çš„é›»å­æ›¸ï¼ˆè‡ªå‹•æ¨¡å¼ï¼‰")
    print("2. åªè™•ç†ç™¾åº¦ç¶²ç›¤")
    print("3. åªè™•ç† Google Drive")
    print("4. è™•ç†æŒ‡å®šæ•¸é‡çš„æ–‡ä»¶ï¼ˆæ¸¬è©¦ï¼‰")
    
    choice = input("\nè«‹é¸æ“‡ (1-4): ").strip()
    
    if choice == '1':
        pipeline.process_cloud_ebooks()
    
    elif choice == '2':
        pipeline.process_cloud_ebooks(sources=['baidu'])
    
    elif choice == '3':
        pipeline.process_cloud_ebooks(sources=['gdrive'])
    
    elif choice == '4':
        try:
            num = int(input("è™•ç†å¤šå°‘å€‹æ–‡ä»¶ï¼Ÿ "))
            pipeline.process_cloud_ebooks(max_files=num)
        except ValueError:
            print("ç„¡æ•ˆçš„æ•¸å­—")
    
    else:
        print("ç„¡æ•ˆçš„é¸æ“‡")


if __name__ == "__main__":
    main()
