import json
import os
import time
from pathlib import Path
from collections import Counter
import datetime

def generate_report():
    print("æ­£åœ¨ç”Ÿæˆå…¨é‡æ·±åº¦æ¢æ¸¬å ±å‘Š...")
    
    # Paths
    base_dir = Path("/home/sms/ebook-converter")
    data_dir = base_dir / "data"
    categories_file = data_dir / "baidu-analysis/file_categories.json"
    mongo_backup = data_dir / "mongodb-backup.jsonl"
    output_dir = data_dir / "markdown-output"
    report_file = Path("/home/sms/baidu_probe_report.md")
    
    # 1. Load Baidu Content Analysis
    if not categories_file.exists():
        print(f"Error: {categories_file} not found.")
        return

    with open(categories_file, 'r', encoding='utf-8') as f:
        categories = json.load(f)
    
    # Calculate totals for ALL categories
    all_files = []
    category_stats = {}
    
    grand_total_size = 0
    grand_total_files = 0
    
    # Categories to include
    target_cats = ['txt_files', 'pdf_text', 'epub_mobi', 'doc_files', 'other_ebooks', 'pdf_scan']
    
    for cat in target_cats:
        file_list = categories.get(cat, [])
        count = len(file_list)
        size = sum(f.get('size', 0) for f in file_list)
        
        category_stats[cat] = {
            'count': count,
            'size': size
        }
        
        grand_total_files += count
        grand_total_size += size
        all_files.extend(file_list)
        
    # TXT specific for current pipeline status
    txt_files = categories.get('txt_files', [])
    txt_total = len(txt_files)
    
    # 2. Load Conversion Progress
    processed_records = []
    if mongo_backup.exists():
        with open(mongo_backup, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    processed_records.append(json.loads(line))
                except:
                    pass
    
    # 3. Analyze Progress
    status_counts = Counter(r.get('status', 'unknown') for r in processed_records)
    processed_filenames = set(r.get('filename') for r in processed_records if r.get('status') == 'success')
    
    # 4. Check Actual Output
    actual_markdown_files = list(output_dir.rglob("*.md"))
    actual_count = len(actual_markdown_files)
    actual_size = sum(f.stat().st_size for f in actual_markdown_files)
    
    # 5. Calculate Metrics (TXT Only for now)
    success_count = status_counts.get('success', 0)
    failed_count = status_counts.get('failed', 0)
    skipped_count = status_counts.get('skipped', 0)
    
    unique_success = len(processed_filenames)
    # Remaining for TXT pipeline
    remaining_txt = txt_total - unique_success
    
    # Overall Progress (against ALL files)
    total_progress_percent = (unique_success / grand_total_files * 100) if grand_total_files > 0 else 0
    
    # 6. Generate Report Content
    lines = []
    lines.append(f"# ç™¾åº¦ç¶²ç›¤å…¨é‡å…§å®¹æ·±åº¦æ¢æ¸¬å ±å‘Š")
    lines.append(f"**ç”Ÿæˆæ™‚é–“**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    lines.append(f"## 1. å…¨é‡çŸ¥è­˜åº«æ¦‚è¦½ (çœŸå¯¦è¦æ¨¡)")
    lines.append(f"> âš ï¸ **ç™¼ç¾**: ä¹‹å‰åƒ…çµ±è¨ˆäº† TXT æ ¼å¼ã€‚å¯¦éš›ä¸Šé‚„æœ‰å¤§é‡ PDF å’Œå…¶ä»–é›»å­æ›¸æ ¼å¼ã€‚")
    lines.append(f"- **ç¸½æ–‡ä»¶æ•¸**: **{grand_total_files:,}**")
    lines.append(f"- **ç¸½é«”ç©**: **{grand_total_size / (1024*1024*1024):.2f} GB**")
    
    lines.append(f"\n### æ ¼å¼åˆ†ä½ˆè©³æƒ…")
    lines.append(f"| æ ¼å¼é¡å‹ | æ–‡ä»¶æ•¸é‡ | ç¸½å¤§å° | è™•ç†é›£åº¦ | ç•¶å‰ç‹€æ…‹ |")
    lines.append(f"|---|---|---|---|---|")
    
    # Map for friendly names
    cat_map = {
        'txt_files': ('ç´”æ–‡æœ¬ (TXT)', 'â­â­ (æ˜“)', 'ğŸ”¥ é€²è¡Œä¸­'),
        'pdf_text': ('PDF (æ–‡å­—ç‰ˆ)', 'â­â­â­â­ (ä¸­)', 'â³ å¾…å•Ÿå‹•'),
        'pdf_scan': ('PDF (æƒæç‰ˆ)', 'â­â­â­â­â­ (é›£)', 'â³ å¾…å•Ÿå‹•'),
        'epub_mobi': ('EPUB/MOBI', 'â­â­â­ (ä¸­)', 'â³ å¾…å•Ÿå‹•'),
        'doc_files': ('Word (DOC/DOCX)', 'â­â­â­ (ä¸­)', 'â³ å¾…å•Ÿå‹•'),
        'other_ebooks': ('å…¶ä»–æ ¼å¼', 'â­â­â­', 'â³ å¾…å•Ÿå‹•')
    }
    
    for cat in target_cats:
        stats = category_stats.get(cat)
        if stats and stats['count'] > 0:
            name, diff, status = cat_map.get(cat, (cat, 'æœªçŸ¥', 'å¾…å®š'))
            size_str = f"{stats['size'] / (1024*1024):.2f} MB"
            if stats['size'] > 1024*1024*1024:
                size_str = f"{stats['size'] / (1024*1024*1024):.2f} GB"
                
            lines.append(f"| {name} | {stats['count']:,} | {size_str} | {diff} | {status} |")
    
    lines.append(f"\n## 2. ç•¶å‰è™•ç†é€²åº¦ (åƒ… TXT ç®¡é“)")
    lines.append(f"- **TXT è½‰åŒ–é€²åº¦**: {unique_success}/{txt_total} ({(unique_success/txt_total*100):.2f}%)")
    lines.append(f"- **å¯¦éš›ç”¢å‡º**: {actual_count:,} å€‹ Markdown æ–‡ä»¶ ({actual_size / (1024*1024):.2f} MB)")
    
    # Progress Bar (TXT)
    bar_len = 20
    txt_percent = (unique_success / txt_total * 100) if txt_total > 0 else 0
    filled = int(txt_percent / 100 * bar_len)
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_len - filled)
    lines.append(f"`TXTç®¡é“: [{bar}] {txt_percent:.2f}%`")

    lines.append(f"\n## 3. ä¸‹ä¸€æ­¥æ“´å±•è¨ˆåŠƒ")
    lines.append(f"1. **PDF è™•ç† (é‡é»)**: ç™¼ç¾ 5000+ å€‹ PDF æ–‡ä»¶ï¼Œéœ€éƒ¨ç½² OCR/PDF è§£æç®¡é“ (å¦‚ pdfplumber/PaddleOCR)ã€‚")
    lines.append(f"2. **EPUB/MOBI è§£æ**: ä½¿ç”¨ ebooklib é€²è¡Œçµæ§‹åŒ–æå–ã€‚")
    lines.append(f"3. **Word è§£æ**: ä½¿ç”¨ python-docx æå–ã€‚")

    # 4. Detailed folder breakdown (keep top 20)
    lines.append(f"\n## 4. é ‚å±¤ç›®éŒ„å…§å®¹åˆ†ä½ˆ (Top 20)")
    folder_counts = Counter()
    for f in all_files:
        path_parts = f.get('full_path', '').split('/')
        if len(path_parts) > 2:
            top_folder = path_parts[1]
            folder_counts[top_folder] += 1
            
    lines.append(f"| ç›®éŒ„ | æ–‡ä»¶æ•¸ |")
    lines.append(f"|---|---|")
    for folder, count in folder_counts.most_common(20):
        lines.append(f"| {folder} | {count:,} |")

    # Write Report
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
        
    print(f"å…¨é‡å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
    print("\n" + "\n".join(lines[:35]))

if __name__ == "__main__":
    generate_report()
