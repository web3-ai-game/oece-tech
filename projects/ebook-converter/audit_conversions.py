import json
import os
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger("Audit")

def audit_conversions():
    print("ğŸ” å•Ÿå‹•è½‰åŒ–è³ªé‡å¯©è¨ˆç¨‹åº...")
    
    base_dir = Path("/home/sms/ebook-converter")
    categories_file = base_dir / "data/baidu-analysis/file_categories.json"
    output_dir = base_dir / "data/markdown-output"
    
    if not categories_file.exists():
        print("âŒ æ‰¾ä¸åˆ°æºæ–‡ä»¶åˆ—è¡¨")
        return

    with open(categories_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
        
    txt_files = data.get('txt_files', [])
    pdf_files = data.get('pdf_text', []) # Assuming text PDFs for now, scan PDFs are different
    
    suspicious_files = []
    
    # 1. Audit TXT Files
    print(f"æ­£åœ¨æª¢æŸ¥ {len(txt_files)} å€‹ TXT æºæ–‡ä»¶...")
    for file_info in txt_files:
        original_size = file_info.get('size', 0)
        if original_size == 0: continue
            
        # Construct expected markdown path
        # Logic matches pipeline: output_dir / relative_path / filename.md
        rel_path = file_info.get('path', '').strip('/')
        if rel_path.startswith("çŸ¥è­˜åº«/"):
            rel_path = rel_path[4:]
            
        md_filename = Path(file_info['name']).stem + ".md"
        md_path = output_dir / rel_path / md_filename
        
        if not md_path.exists():
            # Try finding it recursively if path mapping is tricky
            # For speed, skip this or assume missing
            continue
            
        md_size = md_path.stat().st_size
        
        # Ratio check: If MD size is less than 20% of original TXT size, flag it
        ratio = md_size / original_size
        if ratio < 0.2 and original_size > 1024: # Ignore very small files
            suspicious_files.append({
                'name': file_info['name'],
                'type': 'txt_ratio',
                'original_size': original_size,
                'md_size': md_size,
                'ratio': ratio,
                'path': str(md_path)
            })
            continue

        # Content Quality Check (Garbage Detection)
        try:
            with open(md_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
                
                # Check 1: Mojibake density
                # If replacement char appears frequently (e.g. > 1% of chars or > 50 times absolute)
                # \ufffd is the unicode replacement character
                if content.count('\ufffd') > 50 or (len(content) > 0 and content.count('\ufffd') / len(content) > 0.01):
                    suspicious_files.append({
                        'name': file_info['name'],
                        'type': 'mojibake',
                        'original_size': original_size,
                        'md_size': md_size,
                        'ratio': ratio,
                        'details': f"Found {content.count('\ufffd')} replacement chars",
                        'path': str(md_path)
                    })
                    continue
                    
                # Check 2: Too short (Absolute)
                if len(content) < 200 and original_size > 1024:
                    suspicious_files.append({
                        'name': file_info['name'],
                        'type': 'too_short',
                        'original_size': original_size,
                        'md_size': md_size,
                        'ratio': ratio,
                        'details': f"Content length: {len(content)} chars",
                        'path': str(md_path)
                    })
                    continue

        except Exception as e:
            pass

    # 2. Audit PDF Files (Simple check)
    print(f"æ­£åœ¨æª¢æŸ¥å·²è½‰æ›çš„ PDF æ–‡ä»¶...")
    # Since we don't have a direct map of all processed PDFs easily without DB, 
    # lets scan output dir for known PDF-derived MDs or just check all MDs
    
    # Report results
    print("\n" + "="*60)
    print("ğŸ“Š å¯©è¨ˆå ±å‘Š")
    print("="*60)
    
    if not suspicious_files:
        print("âœ… æ²’æœ‰ç™¼ç¾æ˜é¡¯çš„ç•°å¸¸æ–‡ä»¶ (åŸºæ–¼å¤§å°æ¯”å°)")
    else:
        print(f"âš ï¸ ç™¼ç¾ {len(suspicious_files)} å€‹å¯ç–‘æ–‡ä»¶ (è½‰åŒ–å¾Œé«”ç©éå°):")
        print(f"{'æ–‡ä»¶å':<40} | {'åŸå¤§å°':<10} | {'MDå¤§å°':<10} | {'æ¯”ä¾‹':<6}")
        print("-" * 80)
        
        for f in suspicious_files[:20]: # Show top 20
            print(f"{f['name'][:38]:<40} | {f['original_size']/1024:.1f}KB   | {f['md_size']/1024:.1f}KB   | {f['ratio']:.2%}")
            
        if len(suspicious_files) > 20:
            print(f"... ä»¥åŠå…¶ä»– {len(suspicious_files)-20} å€‹æ–‡ä»¶")
            
    # Save list for reprocessing
    if suspicious_files:
        with open(base_dir / "reprocess_list.json", 'w', encoding='utf-8') as f:
            json.dump(suspicious_files, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å¯ç–‘æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜è‡³: {base_dir}/reprocess_list.json")

if __name__ == "__main__":
    audit_conversions()
