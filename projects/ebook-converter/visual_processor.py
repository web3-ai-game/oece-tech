#!/usr/bin/env python3
"""
å¯è¦–åŒ–æµå¼è™•ç†ç®¡é“ (Rich UI) - ç…‰é‡‘è¡“å£«æ¨¡å‹ V3
å¯¦æ™‚ç›£æ§è½‰æ›é€²åº¦ï¼Œæ”¯æŒå­—ç¬¦ç´šæµå¼é¡¯ç¤º
"""
import os
import json
import time
import logging
import threading
import queue
from pathlib import Path
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from rich.console import Console
from rich.layout import Layout
from rich.panel import Panel
from rich.live import Live
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn
from rich.table import Table
from rich.text import Text
from rich.syntax import Syntax

from dedup_processor import DedupProcessor
from main import EbookConverterPipeline
from gemini_converter import GeminiConverter
from multi_cloud_downloader import MultiCloudDownloader

# é…ç½®æ—¥èªŒæ–‡ä»¶ï¼Œä½†ä¸åœ¨æ§åˆ¶å°è¼¸å‡ºï¼Œä»¥å…å¹²æ“¾ Rich UI
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='/tmp/visual_pipeline.log',
    filemode='w'
)
logger = logging.getLogger(__name__)

class VisualPipelineProcessor:
    def __init__(self, 
                 cache_dir="/home/sms/ebook-converter/data/pipeline-cache",
                 batch_size=100,
                 max_cache_gb=80):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.batch_size = batch_size
        self.max_cache_bytes = max_cache_gb * 1024 * 1024 * 1024
        
        self.analysis_dir = Path("/home/sms/ebook-converter/data/baidu-analysis")
        self.categories_file = self.analysis_dir / "file_categories.json"
        
        self.dedup = DedupProcessor()
        self.downloader = MultiCloudDownloader(cache_dir=str(self.cache_dir))
        
        # åˆå§‹åŒ– pipeline ä½†æ›¿æ› gemini ç‚ºæµå¼
        self.converter_pipeline = EbookConverterPipeline()
        self.gemini = self.converter_pipeline.gemini # è¤‡ç”¨å¯¦ä¾‹
        self.indexer = self.converter_pipeline.indexer # è¤‡ç”¨ç´¢å¼•å™¨
        
        self.mongo_backup = Path("/home/sms/ebook-converter/data/mongodb-backup.jsonl")
        
        self.stats = {
            'total': 0, 'success': 0, 'failed': 0, 'skipped': 0, 
            'chars_processed': 0, 'files_processing': 0
        }
        
        # ç”¨æ–¼ UI é€šä¿¡çš„éšŠåˆ—
        self.log_queue = queue.Queue()
        self.stream_queue = queue.Queue() # ç”¨æ–¼é¡¯ç¤ºå¯¦æ™‚æ–‡æœ¬æµ
        
        # å¯¦æ™‚æµæ—¥èªŒæ–‡ä»¶
        self.stream_log_file = Path("/home/sms/ebook-converter/data/live_stream.log")
        self.stream_log_file.parent.mkdir(parents=True, exist_ok=True)
        # Touch file immediately
        with open(self.stream_log_file, 'w') as f:
            f.write("=== å¯¦æ™‚è½‰æ›æµå•Ÿå‹• ===\n")
            
    def load_txt_files(self) -> List[Dict]:
        with open(self.categories_file, 'r', encoding='utf-8') as f:
            categories = json.load(f)
        return categories.get('txt_files', [])

    def read_file_safe(self, file_path: Path) -> str:
        encodings = ['utf-8', 'gb18030', 'gbk', 'big5', 'latin1']
        for enc in encodings:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    return f.read()
            except: continue
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()

    def process_single_file_stream(self, file_info: Dict, task_id, progress, dry_run=False):
        filename = file_info['name']
        try:
            # 1. ä¸‹è¼‰ (æ¨¡æ“¬æˆ–å¯¦éš›)
            if dry_run:
                # Dry run æ¨¡å¼ï¼šæ¨¡æ“¬å»¶é²å’Œå…§å®¹
                time.sleep(1)
                content = f"# {filename}\n\né€™æ˜¯ä¸€å€‹æ¸¬è©¦å…§å®¹ï¼Œç”¨æ–¼æ¼”ç¤ºæµå¼å‚³è¼¸æ•ˆæœã€‚\n\n## ç« ç¯€ä¸€\n\næ¸¬è©¦æ–‡æœ¬..." * 5
                self.log_queue.put(f"[blue][Dry Run] æ¨¡æ“¬è®€å–[/]: {filename}")
                local_path = self.cache_dir / f"mock_{filename}"
            else:
                # ä½¿ç”¨ MultiCloudDownloader ä¸‹è¼‰
                # ç¢ºä¿ file_info åŒ…å« source
                if 'source' not in file_info:
                    file_info['source'] = 'baidu' # é»˜èªå‡è¨­ç‚ºç™¾åº¦ç¶²ç›¤
                
                local_path_str = self.downloader.download_file(file_info)
                
                if not local_path_str or not Path(local_path_str).exists():
                    raise Exception("ä¸‹è¼‰å¤±æ•—")
                
                local_path = Path(local_path_str)
                content = self.read_file_safe(local_path)
                self.log_queue.put(f"[blue]è®€å–å®Œæˆ[/]: {filename} ({len(content)} å­—ç¬¦)")
            
            # 3. æµå¼è½‰æ›
            metadata = {'filename': filename, 'type': 'txt'}
            markdown_content = ""
            
            if dry_run:
                # æ¨¡æ“¬æµå¼è¼¸å‡º
                for char in content:
                    markdown_content += char
                    self.stream_queue.put(char)
                    progress.advance(task_id, advance=1)
                    time.sleep(0.001) # æ¨¡æ“¬ç”Ÿæˆé€Ÿåº¦
            else:
                # ä½¿ç”¨æµå¼æ¥å£
                stream = self.gemini.convert_text_to_markdown_stream(content, metadata)
                
                for chunk in stream:
                    markdown_content += chunk
                    self.stream_queue.put(chunk) # ç™¼é€åˆ° UI
                    # æ›´æ–°é€²åº¦æ¢
                    progress.advance(task_id, advance=len(chunk))
            
            # 4. ä¿å­˜
            md_path = Path("/home/sms/ebook-converter/data/markdown-output") / f"{Path(filename).stem}.md"
            if not dry_run:
                md_path.parent.mkdir(parents=True, exist_ok=True)
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
            
            # 5. æå–çµæ§‹èˆ‡ç´¢å¼• (æ–°å¢)
            self.log_queue.put(f"[yellow]æå–çµæ§‹[/]: {filename}")
            if dry_run:
                time.sleep(0.5)
                structure = {"propositions": [], "mock": True}
            else:
                structure = self.gemini.extract_structure(markdown_content, filename)
                
                self.log_queue.put(f"[cyan]å»ºç«‹ç´¢å¼•[/]: {filename}")
                self.indexer.add_document(str(local_path), str(md_path), structure)
                self.indexer.save_index() # å¯¦æ™‚ä¿å­˜
            
            self.stats['success'] += 1
            self.log_queue.put(f"[green]è™•ç†å®Œæˆ[/]: {filename}")
            
            # 6. å‚™ä»½
            if not dry_run:
                record = {
                    'filename': filename,
                    'status': 'success',
                    'timestamp': time.time()
                }
                with open(self.mongo_backup, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(record, ensure_ascii=False) + '\n')
                    
                # æ¸…ç†ä¸‹è¼‰çš„æ–‡ä»¶
                if local_path.exists():
                    os.remove(local_path)
            
        except Exception as e:
            self.stats['failed'] += 1
            self.log_queue.put(f"[red]å¤±æ•—[/]: {filename} - {e}")
        finally:
            self.stats['files_processing'] -= 1

    def run(self, max_files=None, max_workers=100, dry_run=False):
        console = Console(force_terminal=True)
        layout = Layout()
        
        layout.split(
            Layout(name="header", size=3),
            Layout(name="main", ratio=1),
            Layout(name="footer", size=10)
        )
        layout["main"].split_row(
            Layout(name="progress"),
            Layout(name="stream", ratio=2)
        )
        
        # Load files or mock files for dry run
        if dry_run and not self.categories_file.exists():
            files = [{'name': f'mock_book_{i}.txt', 'full_path': f'/mock/path/{i}', 'size': 1000} for i in range(10)]
            self.log_queue.put("[yellow]Dry Run: ä½¿ç”¨æ¨¡æ“¬æ–‡ä»¶åˆ—è¡¨[/]")
        else:
            files = self.load_txt_files()
            
        new_files = self.dedup.filter_new_files(files)
        if max_files: new_files = new_files[:max_files]
        
        self.stats['total'] = len(new_files)
        
        # é€²åº¦æ¢
        overall_progress = Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn()
        )
        overall_task = overall_progress.add_task("[yellow]ç¸½é€²åº¦", total=len(new_files))
        
        # å¯¦æ™‚æ–‡æœ¬ç·©è¡å€
        stream_buffer = [""] * 20
        
        with Live(layout, refresh_per_second=10, screen=True) as live:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = []
                # å•Ÿå‹•ä»»å‹™
                for f in new_files:
                    self.stats['files_processing'] += 1
                    # å‚³é dry_run åƒæ•¸
                    fut = executor.submit(self.process_single_file_stream, f, overall_task, overall_progress, dry_run)
                    futures.append(fut)
                    
                    # æ›´æ–° UI (Loop logic same as before, simplified copy here)
                    # We need to process queues frequently to avoid blocking
                    start_time = time.time()
                    while time.time() - start_time < 0.1: # Process queues for a bit
                        while not self.log_queue.empty():
                            msg = self.log_queue.get()
                            # Optional: Display logs in footer or separate panel
                            
                        while not self.stream_queue.empty():
                            chunk = self.stream_queue.get()
                            with open(self.stream_log_file, 'a', encoding='utf-8') as log_f:
                                log_f.write(chunk)
                            
                            last_line = stream_buffer[-1] + chunk
                            if "\n" in last_line:
                                parts = last_line.split("\n")
                                stream_buffer[-1] = parts[0]
                                stream_buffer.extend(parts[1:])
                                stream_buffer = stream_buffer[-20:]
                            else:
                                stream_buffer[-1] = last_line
                        
                        layout["header"].update(Panel(f"ğŸš€ ç…‰é‡‘è¡“å£«å¼•æ“ | ç¸½ä»»å‹™: {self.stats['total']} | è™•ç†ä¸­: {self.stats['files_processing']} | æˆåŠŸ: {self.stats['success']} | æ¨¡å¼: {'Dry Run' if dry_run else 'Production'}", style="bold blue"))
                        layout["progress"].update(Panel(overall_progress, title="ä»»å‹™éšŠåˆ—"))
                        text_display = "\n".join(stream_buffer)
                        layout["stream"].update(Panel(Syntax(text_display, "markdown"), title="å¯¦æ™‚è½‰æ›æµ (Live Stream)"))
                        overall_progress.update(overall_task, advance=0)
                    
                    # Limit submission rate slightly
                    time.sleep(0.05)
                
                # ç­‰å¾…å®Œæˆ
                while any(f.running() for f in futures):
                     # Process queues
                    while not self.log_queue.empty(): self.log_queue.get()
                    while not self.stream_queue.empty():
                        chunk = self.stream_queue.get()
                        last_line = stream_buffer[-1] + chunk
                        if "\n" in last_line:
                            parts = last_line.split("\n")
                            stream_buffer[-1] = parts[0]
                            stream_buffer.extend(parts[1:])
                            stream_buffer = stream_buffer[-20:]
                        else:
                            stream_buffer[-1] = last_line
                            
                    completed_count = sum(1 for f in futures if f.done())
                    overall_progress.update(overall_task, completed=completed_count)
                    
                    layout["header"].update(Panel(f"ğŸš€ ç…‰é‡‘è¡“å£«å¼•æ“ | ç¸½ä»»å‹™: {self.stats['total']} | è™•ç†ä¸­: {self.stats['files_processing']} | æˆåŠŸ: {self.stats['success']}", style="bold blue"))
                    text_display = "\n".join(stream_buffer)
                    layout["stream"].update(Panel(Syntax(text_display, "markdown"), title="å¯¦æ™‚è½‰æ›æµ (Live Stream)"))
                    
                    time.sleep(0.1)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', '--num', type=int, default=500)
    parser.add_argument('-w', '--workers', type=int, default=50)
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ“¬é‹è¡Œï¼Œä¸å¯¦éš›ä¸‹è¼‰å’Œèª¿ç”¨API')
    args = parser.parse_args()
    
    processor = VisualPipelineProcessor()
    processor.run(max_files=args.num, max_workers=args.workers, dry_run=args.dry_run)
