#!/usr/bin/env python3
"""è¶…é«˜ä¸¦ç™¼è™•ç†å™¨ - é›™ API Key è² è¼‰å‡è¡¡ + æœ€å¤§ä¸¦ç™¼"""
import os
import sys
import time
import random
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from multi_cloud_downloader import MultiCloudDownloader
from main import EbookConverterPipeline
import logging
import threading
from queue import Queue

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(threadName)-10s] - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/ultra-processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class APIKeyBalancer:
    """API Key è² è¼‰å‡è¡¡å™¨"""
    
    def __init__(self, api_keys):
        self.api_keys = api_keys
        self.current_index = 0
        self.lock = threading.Lock()
        self.usage_count = {key: 0 for key in api_keys}
    
    def get_key(self):
        """è¼ªè©¢ç²å– API Key"""
        with self.lock:
            key = self.api_keys[self.current_index]
            self.usage_count[key] += 1
            self.current_index = (self.current_index + 1) % len(self.api_keys)
            return key
    
    def get_stats(self):
        """ç²å–ä½¿ç”¨çµ±è¨ˆ"""
        with self.lock:
            return dict(self.usage_count)


class UltraParallelProcessor:
    """è¶…é«˜ä¸¦ç™¼è™•ç†å™¨"""
    
    def __init__(self, api_keys, max_workers=20):
        """
        åˆå§‹åŒ–è¶…é«˜ä¸¦ç™¼è™•ç†å™¨
        
        api_keys: API Key åˆ—è¡¨
        max_workers: æœ€å¤§ä¸¦ç™¼æ•¸ï¼ˆå»ºè­° 15-30ï¼‰
        """
        self.api_keys = api_keys
        self.key_balancer = APIKeyBalancer(api_keys)
        self.downloader = MultiCloudDownloader()
        self.max_workers = max_workers
        
        self.stats_lock = threading.Lock()
        self.stats = {
            'processed': 0,
            'failed': 0,
            'total_size': 0,
            'start_time': time.time()
        }
        
        logger.info(f"è¶…é«˜ä¸¦ç™¼è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"API Keys: {len(api_keys)} å€‹")
        logger.info(f"ä¸¦ç™¼æ•¸: {max_workers} å€‹ç·šç¨‹")
        logger.info(f"é è¨ˆè™•ç†é€Ÿåº¦: {max_workers * 2} æ–‡ä»¶/åˆ†é˜")
    
    def process_single_file(self, file_info):
        """è™•ç†å–®å€‹æ–‡ä»¶ï¼ˆä½¿ç”¨è² è¼‰å‡è¡¡çš„ API Keyï¼‰"""
        filename = file_info['name']
        thread_name = threading.current_thread().name
        
        try:
            # ç²å– API Key
            api_key = self.key_balancer.get_key()
            
            logger.info(f"[{thread_name}] é–‹å§‹è™•ç†: {filename[:50]}...")
            
            # 1. ä¸‹è¼‰
            local_path = self.downloader.download_file(file_info)
            if not local_path or not os.path.exists(local_path):
                logger.error(f"[{thread_name}] ä¸‹è¼‰å¤±æ•—: {filename}")
                return {'success': False, 'filename': filename, 'error': 'download_failed'}
            
            file_size = os.path.getsize(local_path)
            
            # 2. è½‰æ›ï¼ˆä½¿ç”¨æŒ‡å®šçš„ API Keyï¼‰
            import config
            original_key = config.GEMINI_API_KEY
            config.GEMINI_API_KEY = api_key
            
            converter = EbookConverterPipeline()
            success = converter.process_single_file(local_path)
            
            # æ¢å¾©åŸå§‹ Key
            config.GEMINI_API_KEY = original_key
            
            # 3. æ¸…ç†
            try:
                os.remove(local_path)
            except:
                pass
            
            # 4. æ›´æ–°çµ±è¨ˆ
            with self.stats_lock:
                if success:
                    self.stats['processed'] += 1
                    self.stats['total_size'] += file_size
                else:
                    self.stats['failed'] += 1
            
            if success:
                logger.info(f"[{thread_name}] âœ“ å®Œæˆ: {filename[:50]} ({file_size/1024:.1f} KB)")
                return {'success': True, 'filename': filename, 'size': file_size}
            else:
                logger.error(f"[{thread_name}] âœ— è½‰æ›å¤±æ•—: {filename}")
                return {'success': False, 'filename': filename, 'error': 'conversion_failed'}
                
        except Exception as e:
            logger.error(f"[{thread_name}] è™•ç†å¤±æ•— {filename}: {e}")
            with self.stats_lock:
                self.stats['failed'] += 1
            return {'success': False, 'filename': filename, 'error': str(e)}
    
    def process_all_ebooks(self, max_files=None, sources=['gdrive', 'baidu']):
        """è¶…é«˜ä¸¦ç™¼è™•ç†æ‰€æœ‰é›»å­æ›¸"""
        logger.info("=" * 80)
        logger.info("ğŸš€ è¶…é«˜ä¸¦ç™¼è™•ç†å™¨å•Ÿå‹•")
        logger.info("=" * 80)
        logger.info(f"API Keys: {len(self.api_keys)} å€‹")
        logger.info(f"ä¸¦ç™¼ç·šç¨‹: {self.max_workers} å€‹")
        logger.info(f"è™•ç†ä¾†æº: {', '.join(sources)}")
        logger.info("=" * 80)
        
        # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
        logger.info("\næƒæé›²ç›¤...")
        all_files = []
        
        if 'baidu' in sources:
            baidu_files = self.downloader.list_baidu_files("/apps/bypy")
            all_files.extend(baidu_files)
            logger.info(f"ç™¾åº¦ç¶²ç›¤: {len(baidu_files)} å€‹æ–‡ä»¶")
        
        if 'gdrive' in sources:
            gdrive_files = self.downloader.list_gdrive_files("")
            all_files.extend(gdrive_files)
            logger.info(f"Google Drive: {len(gdrive_files)} å€‹æ–‡ä»¶")
        
        if not all_files:
            logger.warning("æœªæ‰¾åˆ°é›»å­æ›¸æ–‡ä»¶")
            return
        
        if max_files:
            all_files = all_files[:max_files]
        
        logger.info(f"\næº–å‚™è™•ç† {len(all_files)} å€‹é›»å­æ›¸")
        logger.info(f"é è¨ˆè€—æ™‚: {len(all_files) / (self.max_workers * 2):.1f} åˆ†é˜")
        logger.info("")
        
        # è¶…é«˜ä¸¦ç™¼è™•ç†
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.process_single_file, file_info): file_info
                for file_info in all_files
            }
            
            completed = 0
            for future in as_completed(futures):
                completed += 1
                file_info = futures[future]
                
                try:
                    result = future.result()
                    results.append(result)
                    
                    # æ¯ 10 å€‹æ–‡ä»¶é¡¯ç¤ºé€²åº¦
                    if completed % 10 == 0:
                        with self.stats_lock:
                            elapsed = time.time() - self.stats['start_time']
                            speed = self.stats['processed'] / (elapsed / 60) if elapsed > 0 else 0
                            logger.info(f"\né€²åº¦: {completed}/{len(all_files)} | "
                                      f"æˆåŠŸ: {self.stats['processed']} | "
                                      f"å¤±æ•—: {self.stats['failed']} | "
                                      f"é€Ÿåº¦: {speed:.1f} æ–‡ä»¶/åˆ†é˜\n")
                
                except Exception as e:
                    logger.error(f"ä»»å‹™ç•°å¸¸: {e}")
                    results.append({
                        'success': False,
                        'filename': file_info['name'],
                        'error': str(e)
                    })
        
        # ç¸½çµ
        self.print_summary(results)
        return results
    
    def print_summary(self, results):
        """æ‰“å°è™•ç†ç¸½çµ"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š è™•ç†ç¸½çµ")
        logger.info("=" * 80)
        
        with self.stats_lock:
            elapsed = time.time() - self.stats['start_time']
            speed = self.stats['processed'] / (elapsed / 60) if elapsed > 0 else 0
            
            logger.info(f"\nâœ… æˆåŠŸ: {self.stats['processed']} å€‹")
            logger.info(f"âŒ å¤±æ•—: {self.stats['failed']} å€‹")
            logger.info(f"ğŸ“¦ ç¸½å¤§å°: {self.stats['total_size']/1024/1024:.1f} MB")
            logger.info(f"â±ï¸  è€—æ™‚: {elapsed/60:.1f} åˆ†é˜")
            logger.info(f"âš¡ é€Ÿåº¦: {speed:.1f} æ–‡ä»¶/åˆ†é˜")
        
        # API Key ä½¿ç”¨çµ±è¨ˆ
        logger.info(f"\nğŸ“ˆ API Key ä½¿ç”¨çµ±è¨ˆ:")
        stats = self.key_balancer.get_stats()
        for i, (key, count) in enumerate(stats.items(), 1):
            logger.info(f"  Key {i}: {count} æ¬¡è«‹æ±‚")
        
        logger.info("\n" + "=" * 80)
        logger.info("æŸ¥çœ‹çµæœ:")
        logger.info("  Markdown: /home/sms/ebook-converter/data/markdown-output/")
        logger.info("  ç´¢å¼•: /home/sms/ebook-converter/data/wittgenstein-index/")
        logger.info("=" * 80)


def main():
    """ä¸»ç¨‹åº"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¶…é«˜ä¸¦ç™¼é›»å­æ›¸è™•ç†å™¨')
    parser.add_argument('-w', '--workers', type=int, default=20,
                        help='ä¸¦ç™¼ç·šç¨‹æ•¸ (é»˜èª: 20)')
    parser.add_argument('-n', '--num-files', type=int, default=None,
                        help='è™•ç†æ–‡ä»¶æ•¸é‡ (é»˜èª: å…¨éƒ¨)')
    parser.add_argument('-s', '--sources', nargs='+', 
                        default=['gdrive', 'baidu'],
                        help='è™•ç†ä¾†æº (é»˜èª: gdrive baidu)')
    
    args = parser.parse_args()
    
    # é›™ API Key
    api_keys = [
        "AIzaSyCG459HOLhXkbDQgw8rSYAvuqyM3RdMQHQ",
        "AIzaSyAuJRsStdO7WAIM5I9NtyUgumUzWPiz43o"
    ]
    
    processor = UltraParallelProcessor(
        api_keys=api_keys,
        max_workers=args.workers
    )
    
    processor.process_all_ebooks(
        max_files=args.num_files,
        sources=args.sources
    )


if __name__ == "__main__":
    main()
