#!/usr/bin/env python3
"""ç™¾åº¦ç¶²ç›¤é›»å­æ›¸åº«åˆ†æå™¨ - å»ºç«‹ç›®éŒ„çµæ§‹ä¸¦åˆ†é¡"""
import os
import subprocess
import json
from pathlib import Path
from typing import List, Dict
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BaiduLibraryAnalyzer:
    """ç™¾åº¦ç¶²ç›¤é›»å­æ›¸åº«åˆ†æå™¨"""
    
    def __init__(self, output_dir="/home/sms/ebook-converter/data/baidu-analysis"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶åˆ†é¡
        self.categories = {
            'txt_files': [],           # TXT æ–‡æœ¬ï¼ˆå„ªå…ˆè™•ç†ï¼‰
            'epub_mobi': [],           # EPUB/MOBIï¼ˆå„ªå…ˆè™•ç†ï¼‰
            'ancient_books': [],       # ç¸£èªŒ/æ–¹èªŒ/å¤ç±ï¼ˆS3 ç´¢å¼•ï¼Œä¸è½‰ Markdownï¼‰
            'pdf_text': [],            # PDF æ–‡å­—ç‰ˆï¼ˆéœ€æª¢æ¸¬ï¼‰
            'pdf_scan': [],            # PDF æƒæç‰ˆï¼ˆæœ€å¾Œè™•ç†ï¼‰
            'doc_files': [],           # DOC/DOCX
            'other_ebooks': [],        # å…¶ä»–é›»å­æ›¸æ ¼å¼
            'junk_files': []           # åƒåœ¾æ–‡ä»¶
        }
        
        # é—œéµè©å®šç¾©
        self.ancient_keywords = ['å¿å¿—', 'æ–¹å¿—', 'å¤ç±', 'æ‰«æ', 'å½±å°', 'å…¨é›†', 'å²æ–™', 'é€šé‰´']
        
        # æ ¼å¼å®šç¾©
        self.priority_formats = ['.txt', '.epub', '.mobi', '.azw', '.azw3']
        self.doc_formats = ['.doc', '.docx', '.rtf', '.odt']
        self.other_formats = ['.djvu', '.fb2', '.md', '.html', '.htm']
        self.junk_formats = ['.exe', '.dll', '.ini', '.db', '.bat', '.chm', '.url', '.lnk']
        
        self.stats = {
            'total_dirs': 0,
            'total_files': 0,
            'total_size': 0
        }
    
    def scan_directory(self, remote_path="/çŸ¥è­˜åº«", max_depth=10, _depth=0) -> Dict:
        """éæ­¸æƒæç™¾åº¦ç¶²ç›¤ç›®éŒ„"""
        if _depth > max_depth:
            logger.warning(f"é”åˆ°æœ€å¤§æ·±åº¦ {max_depth}ï¼Œåœæ­¢æƒæ: {remote_path}")
            return {}
        
        indent = "  " * _depth
        logger.info(f"{indent}ğŸ“‚ æƒæ: {remote_path}")
        
        directory_tree = {
            'path': remote_path,
            'name': os.path.basename(remote_path) or remote_path,
            'type': 'directory',
            'children': [],
            'files': [],
            'stats': {'dirs': 0, 'files': 0, 'size': 0}
        }
        
        try:
            result = subprocess.run(
                ["bypy", "list", remote_path],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            lines = result.stdout.strip().split('\n')
            subdirs = []
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('/apps/bypy') or '($t $f $s $m $d)' in line:
                    continue
                
                parts = line.split()
                if len(parts) < 2:
                    continue
                
                file_type = parts[0]
                filename = parts[1]
                
                # ç›®éŒ„
                if file_type == 'D':
                    subdir_path = f"{remote_path}/{filename}".replace('//', '/')
                    subdirs.append((filename, subdir_path))
                    self.stats['total_dirs'] += 1
                    continue
                
                # æ–‡ä»¶
                if file_type == 'F':
                    size = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0
                    ext = os.path.splitext(filename)[1].lower()
                    
                    file_info = {
                        'name': filename,
                        'source': 'baidu',
                        'path': remote_path,
                        'full_path': f"{remote_path}/{filename}".replace('//', '/'),
                        'size': size,
                        'ext': ext,
                        'type': self._classify_file(filename, ext)
                    }
                    
                    directory_tree['files'].append(file_info)
                    directory_tree['stats']['files'] += 1
                    directory_tree['stats']['size'] += size
                    
                    self.stats['total_files'] += 1
                    self.stats['total_size'] += size
                    
                    # åˆ†é¡
                    self._categorize_file(file_info)
            
            # éæ­¸æƒæå­ç›®éŒ„
            if subdirs:
                logger.info(f"{indent}  ğŸ“ ç™¼ç¾ {len(subdirs)} å€‹å­ç›®éŒ„")
                
            for subdir_name, subdir_path in subdirs:
                try:
                    subtree = self.scan_directory(subdir_path, max_depth, _depth + 1)
                    if subtree:
                        directory_tree['children'].append(subtree)
                        directory_tree['stats']['dirs'] += 1 + subtree['stats']['dirs']
                        directory_tree['stats']['files'] += subtree['stats']['files']
                        directory_tree['stats']['size'] += subtree['stats']['size']
                except Exception as e:
                    logger.warning(f"{indent}  âœ— ç„¡æ³•æƒæ {subdir_name}: {e}")
            
            # é¡¯ç¤ºç•¶å‰ç›®éŒ„çµ±è¨ˆ
            if directory_tree['files'] or directory_tree['children']:
                logger.info(f"{indent}  âœ“ æœ¬å±¤: {len(directory_tree['files'])} æ–‡ä»¶, "
                          f"{len(directory_tree['children'])} å­ç›®éŒ„")
            
            return directory_tree
            
        except subprocess.TimeoutExpired:
            logger.error(f"{indent}âœ— æƒæè¶…æ™‚: {remote_path}")
            return {}
        except Exception as e:
            logger.error(f"{indent}âœ— æƒæå¤±æ•— {remote_path}: {e}")
            return {}
    
    def _classify_file(self, filename: str, ext: str) -> str:
        """åˆ†é¡æ–‡ä»¶é¡å‹"""
        # 1. å„ªå…ˆæª¢æŸ¥æ˜¯å¦ç‚ºå¤ç±/ç¸£èªŒ (åŸºæ–¼æ–‡ä»¶åé—œéµè©)
        if any(kw in filename for kw in self.ancient_keywords):
            return 'ancient'

        if ext in self.priority_formats:
            return 'priority'
        elif ext == '.pdf':
            return 'pdf'
        elif ext in self.doc_formats:
            return 'document'
        elif ext in self.other_formats:
            return 'other_ebook'
        elif ext in self.junk_formats:
            return 'junk'
        else:
            return 'unknown'
    
    def _categorize_file(self, file_info: Dict):
        """å°‡æ–‡ä»¶æ­¸é¡åˆ°å°æ‡‰é¡åˆ¥"""
        ext = file_info['ext']
        file_type = file_info['type']
        
        if file_type == 'ancient':
            self.categories['ancient_books'].append(file_info)
        elif file_type == 'priority':
            if ext == '.txt':
                self.categories['txt_files'].append(file_info)
            else:
                self.categories['epub_mobi'].append(file_info)
        elif file_type == 'pdf':
            # PDF æš«æ™‚éƒ½æ­¸é¡ç‚ºå¾…æª¢æ¸¬
            self.categories['pdf_text'].append(file_info)
        elif file_type == 'document':
            self.categories['doc_files'].append(file_info)
        elif file_type == 'other_ebook':
            self.categories['other_ebooks'].append(file_info)
        elif file_type == 'junk':
            self.categories['junk_files'].append(file_info)
    
    def save_results(self, directory_tree: Dict):
        """ä¿å­˜åˆ†æçµæœ"""
        # 1. ä¿å­˜å®Œæ•´ç›®éŒ„æ¨¹
        tree_file = self.output_dir / "directory_tree.json"
        with open(tree_file, 'w', encoding='utf-8') as f:
            json.dump(directory_tree, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ ç›®éŒ„æ¨¹å·²ä¿å­˜: {tree_file}")
        
        # 2. ä¿å­˜åˆ†é¡çµæœ
        categories_file = self.output_dir / "file_categories.json"
        with open(categories_file, 'w', encoding='utf-8') as f:
            json.dump(self.categories, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ åˆ†é¡çµæœå·²ä¿å­˜: {categories_file}")
        
        # 3. ç”Ÿæˆ Markdown å ±å‘Š
        self._generate_markdown_report(directory_tree)
        
        # 4. ç”Ÿæˆè™•ç†è¨ˆåŠƒ
        self._generate_processing_plan()
    
    def _generate_markdown_report(self, directory_tree: Dict):
        """ç”Ÿæˆ Markdown æ ¼å¼çš„åˆ†æå ±å‘Š"""
        report_file = self.output_dir / "analysis_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ç™¾åº¦ç¶²ç›¤é›»å­æ›¸åº«åˆ†æå ±å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {os.popen('date').read().strip()}\n\n")
            f.write("---\n\n")
            
            # ç¸½é«”çµ±è¨ˆ
            f.write("## ğŸ“Š ç¸½é«”çµ±è¨ˆ\n\n")
            f.write(f"- **ç¸½ç›®éŒ„æ•¸**: {self.stats['total_dirs']}\n")
            f.write(f"- **ç¸½æ–‡ä»¶æ•¸**: {self.stats['total_files']}\n")
            f.write(f"- **ç¸½å¤§å°**: {self.stats['total_size'] / 1024 / 1024 / 1024:.2f} GB\n\n")
            
            # åˆ†é¡çµ±è¨ˆ
            f.write("## ğŸ“š æ–‡ä»¶åˆ†é¡çµ±è¨ˆ\n\n")
            f.write("### ğŸ“œ ç¸£èªŒ/æ–¹èªŒ/å¤ç±ï¼ˆåƒ…ç´¢å¼•ï¼‰\n\n")
            f.write(f"- **æ–‡ä»¶æ•¸**: {len(self.categories['ancient_books'])} å€‹\n")
            f.write("- **ç­–ç•¥**: è·³é OCRï¼Œç›´æ¥ä¸Šå‚³ S3 ä¸¦å»ºç«‹ç´¢å¼•\n\n")

            f.write("### å„ªå…ˆè™•ç†ï¼ˆä¾¿å®œã€å¿«é€Ÿï¼‰\n\n")
            f.write(f"- **TXT æ–‡ä»¶**: {len(self.categories['txt_files'])} å€‹\n")
            f.write(f"- **EPUB/MOBI**: {len(self.categories['epub_mobi'])} å€‹\n\n")
            
            f.write("### æ¨™æº–è™•ç†\n\n")
            f.write(f"- **DOC/DOCX**: {len(self.categories['doc_files'])} å€‹\n")
            f.write(f"- **å…¶ä»–é›»å­æ›¸**: {len(self.categories['other_ebooks'])} å€‹\n\n")
            
            f.write("### PDF æ–‡ä»¶ï¼ˆéœ€é€²ä¸€æ­¥æª¢æ¸¬ï¼‰\n\n")
            f.write(f"- **PDF ç¸½æ•¸**: {len(self.categories['pdf_text'])} å€‹\n")
            f.write(f"- âš ï¸ éœ€è¦æª¢æ¸¬æ˜¯æ–‡å­—ç‰ˆé‚„æ˜¯æƒæç‰ˆ\n\n")
            
            f.write("### åƒåœ¾æ–‡ä»¶\n\n")
            f.write(f"- **åƒåœ¾æ–‡ä»¶**: {len(self.categories['junk_files'])} å€‹\n")
            f.write(f"- æ ¼å¼: {', '.join(self.junk_formats)}\n\n")
            
            # è™•ç†å»ºè­°
            f.write("## ğŸ¯ è™•ç†å»ºè­°\n\n")
            f.write("### ç¬¬ä¸€éšæ®µï¼šå„ªå…ˆè™•ç†ï¼ˆç«‹å³é–‹å§‹ï¼‰\n\n")
            f.write(f"è™•ç† {len(self.categories['txt_files']) + len(self.categories['epub_mobi'])} å€‹ TXT/EPUB æ–‡ä»¶\n")
            f.write("- æˆæœ¬ä½\n")
            f.write("- é€Ÿåº¦å¿«\n")
            f.write("- å¯ä½¿ç”¨ Gemini Flash æ¨¡å‹\n\n")
            
            f.write("### ç¬¬äºŒéšæ®µï¼šæ¨™æº–è™•ç†\n\n")
            f.write(f"è™•ç† {len(self.categories['doc_files'])} å€‹ DOC/DOCX æ–‡ä»¶\n\n")
            
            f.write("### ç¬¬ä¸‰éšæ®µï¼šPDF æª¢æ¸¬èˆ‡åˆ†æµ\n\n")
            f.write(f"æª¢æ¸¬ {len(self.categories['pdf_text'])} å€‹ PDF æ–‡ä»¶\n")
            f.write("- æ–‡å­—ç‰ˆ PDF â†’ ç›´æ¥è™•ç†\n")
            f.write("- æƒæç‰ˆ PDF â†’ æŒ‰éœ€è™•ç†ï¼ˆæˆæœ¬é«˜ï¼‰\n\n")
            
            # ç›®éŒ„çµæ§‹é è¦½
            f.write("## ğŸ“ ç›®éŒ„çµæ§‹é è¦½\n\n")
            f.write("```\n")
            self._write_tree_preview(f, directory_tree, 0, max_depth=3)
            f.write("```\n\n")
            
            # Top 10 æœ€å¤§æ–‡ä»¶
            f.write("## ğŸ“¦ Top 10 æœ€å¤§æ–‡ä»¶\n\n")
            all_files = []
            for category in self.categories.values():
                all_files.extend(category)
            all_files.sort(key=lambda x: x['size'], reverse=True)
            
            for i, file_info in enumerate(all_files[:10], 1):
                size_mb = file_info['size'] / 1024 / 1024
                f.write(f"{i}. **{file_info['name']}** ({size_mb:.1f} MB) - {file_info['type']}\n")
                f.write(f"   - è·¯å¾‘: `{file_info['full_path']}`\n\n")
        
        logger.info(f"âœ“ Markdown å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def _write_tree_preview(self, f, node: Dict, depth: int, max_depth: int):
        """å¯«å…¥ç›®éŒ„æ¨¹é è¦½"""
        if depth > max_depth:
            return
        
        indent = "  " * depth
        f.write(f"{indent}{node['name']}/\n")
        
        # é¡¯ç¤ºæ–‡ä»¶æ•¸é‡
        if node['files']:
            f.write(f"{indent}  ({len(node['files'])} å€‹æ–‡ä»¶)\n")
        
        # éæ­¸é¡¯ç¤ºå­ç›®éŒ„
        for child in node['children'][:5]:  # åªé¡¯ç¤ºå‰5å€‹å­ç›®éŒ„
            self._write_tree_preview(f, child, depth + 1, max_depth)
        
        if len(node['children']) > 5:
            f.write(f"{indent}  ... é‚„æœ‰ {len(node['children']) - 5} å€‹å­ç›®éŒ„\n")
    
    def _generate_processing_plan(self):
        """ç”Ÿæˆè™•ç†è¨ˆåŠƒ"""
        plan_file = self.output_dir / "processing_plan.md"
        
        with open(plan_file, 'w', encoding='utf-8') as f:
            f.write("# é›»å­æ›¸è™•ç†è¨ˆåŠƒ\n\n")
            f.write("## éšæ®µ 0: å¤ç±/ç¸£èªŒæ­¸æª” (Fast Track)\n\n")
            f.write(f"- æ–‡ä»¶æ•¸: {len(self.categories['ancient_books'])}\n")
            f.write("- æ“ä½œ: åƒ…ä¸‹è¼‰ -> S3 å‚™ä»½ -> å»ºç«‹å…ƒæ•¸æ“šç´¢å¼•\n")
            f.write("- é è¨ˆé€Ÿåº¦: æ¥µå¿« (åƒ…å—å¸¶å¯¬é™åˆ¶)\n\n")

            f.write("## éšæ®µ 1: TXT æ–‡ä»¶ï¼ˆå„ªå…ˆï¼‰\n\n")
            f.write(f"- æ–‡ä»¶æ•¸: {len(self.categories['txt_files'])}\n")
            f.write(f"- é è¨ˆæ™‚é–“: {len(self.categories['txt_files']) / 20:.1f} åˆ†é˜ï¼ˆ20æ–‡ä»¶/åˆ†é˜ï¼‰\n")
            f.write(f"- æˆæœ¬: æ¥µä½\n\n")
            
            f.write("## éšæ®µ 2: EPUB/MOBI\n\n")
            f.write(f"- æ–‡ä»¶æ•¸: {len(self.categories['epub_mobi'])}\n")
            f.write(f"- é è¨ˆæ™‚é–“: {len(self.categories['epub_mobi']) / 15:.1f} åˆ†é˜ï¼ˆ15æ–‡ä»¶/åˆ†é˜ï¼‰\n")
            f.write(f"- æˆæœ¬: ä½\n\n")
            
            f.write("## éšæ®µ 3: DOC/DOCX\n\n")
            f.write(f"- æ–‡ä»¶æ•¸: {len(self.categories['doc_files'])}\n")
            f.write(f"- é è¨ˆæ™‚é–“: {len(self.categories['doc_files']) / 10:.1f} åˆ†é˜ï¼ˆ10æ–‡ä»¶/åˆ†é˜ï¼‰\n")
            f.write(f"- æˆæœ¬: ä¸­ç­‰\n\n")
            
            f.write("## éšæ®µ 4: PDF æª¢æ¸¬\n\n")
            f.write(f"- æ–‡ä»¶æ•¸: {len(self.categories['pdf_text'])}\n")
            f.write(f"- éœ€è¦å…ˆæª¢æ¸¬æ˜¯æ–‡å­—ç‰ˆé‚„æ˜¯æƒæç‰ˆ\n")
            f.write(f"- æ–‡å­—ç‰ˆå¯ç›´æ¥è™•ç†ï¼Œæƒæç‰ˆæŒ‰éœ€è™•ç†\n\n")
            
            f.write("## åƒåœ¾æ¸…ç†\n\n")
            f.write(f"- åƒåœ¾æ–‡ä»¶: {len(self.categories['junk_files'])} å€‹\n")
            f.write(f"- å»ºè­°: ç›´æ¥å¿½ç•¥\n\n")
        
        logger.info(f"âœ“ è™•ç†è¨ˆåŠƒå·²ç”Ÿæˆ: {plan_file}")


def main():
    """ä¸»ç¨‹åº"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç™¾åº¦ç¶²ç›¤é›»å­æ›¸åº«åˆ†æå™¨')
    parser.add_argument('-p', '--path', default='/çŸ¥è­˜åº«', help='ç™¾åº¦ç¶²ç›¤è·¯å¾‘')
    parser.add_argument('-d', '--depth', type=int, default=10, help='æœ€å¤§æƒææ·±åº¦')
    
    args = parser.parse_args()
    
    analyzer = BaiduLibraryAnalyzer()
    
    logger.info("=" * 80)
    logger.info("ğŸš€ ç™¾åº¦ç¶²ç›¤é›»å­æ›¸åº«åˆ†æå™¨å•Ÿå‹•")
    logger.info("=" * 80)
    
    # æƒæç›®éŒ„
    directory_tree = analyzer.scan_directory(args.path, max_depth=args.depth)
    
    # ä¿å­˜çµæœ
    if directory_tree:
        analyzer.save_results(directory_tree)
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š åˆ†æå®Œæˆï¼")
        logger.info("=" * 80)
        logger.info(f"ç¸½ç›®éŒ„: {analyzer.stats['total_dirs']}")
        logger.info(f"ç¸½æ–‡ä»¶: {analyzer.stats['total_files']}")
        logger.info(f"ç¸½å¤§å°: {analyzer.stats['total_size'] / 1024 / 1024 / 1024:.2f} GB")
        logger.info(f"\nå ±å‘Šä½ç½®: {analyzer.output_dir}/analysis_report.md")
        logger.info("=" * 80)


if __name__ == "__main__":
    main()
