#!/usr/bin/env python3
"""
ğŸ¯ è«¸è‘›äº®Â·è³½åšè’¸é¤¾å¡” V2.0 ULTIMATE
ä»£è™Ÿ: PROJECT_GOLDEN_ALCHEMY
æ ¸å¿ƒ: Gemini 3.0 Pro Deep Thinking Ã— 4 Key ä½µç™¼çŸ©é™£
"""

import os
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import google.generativeai as genai
from tqdm import tqdm

# ğŸ”¥ 4å€‹æ”¶è²»KeyçŸ©é™£ - å…¨ç«åŠ›è¦†è“‹
GEMINI_KEYS = [
    "AIzaSyA3ikY04T94AoAwndr20QxV9nl4w_NF_u4",
    "AIzaSyAj08QZ4B8CMU_CTG-QtGUEv0gBHZbM_cQ", 
    "AQ.Ab8RN6LioS7k0Ipycl6oKXFuhww6VTXuosXwgeS8VMpTyZUFcw",
    "AQ.Ab8RN6LlrNEKtXonwqhBKhVRziaoBgHiUwE6CpdSv5Ttil4JgA"
]

# é…ç½®
MODEL_NAME = 'gemini-3-pro-preview'  # ğŸ’ æ·±åº¦æ€è€ƒæ¨¡å¼
BUDGET_LIMIT = 10.0  # USD per key
EXCHANGE_RATE = 35.5  # THB/USD
MAX_WORKERS = 4  # 4å€‹å®¹å™¨ä½µç™¼

# ç›®éŒ„
INPUT_DIR = Path(os.getenv('INPUT_DIR', './input'))
OUTPUT_DIR = Path(os.getenv('OUTPUT_DIR', './output'))
CHECKPOINT_DIR = Path('./checkpoints')

class ZhugeDistillationTower:
    """è«¸è‘›äº®Â·è³½åšè’¸é¤¾å¡”"""
    
    def __init__(self):
        self.models = []
        self.costs = [0.0] * len(GEMINI_KEYS)
        self.processed = 0
        self.total_tokens = {'input': 0, 'output': 0}
        
        # åˆå§‹åŒ–4å€‹æ¨¡å‹å¯¦ä¾‹
        for idx, key in enumerate(GEMINI_KEYS):
            genai.configure(api_key=key)
            model = genai.GenerativeModel(MODEL_NAME)
            self.models.append({
                'id': idx + 1,
                'model': model,
                'cost': 0.0,
                'docs': 0
            })
        
        self.checkpoint = self.load_checkpoint()
        
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘       ğŸ¯ è«¸è‘›äº®Â·è³½åšè’¸é¤¾å¡” V2.0 ULTIMATE                        â•‘")
        print("â•‘          PROJECT_GOLDEN_ALCHEMY                                 â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"\nğŸ’ æ¨¡å‹: {MODEL_NAME}")
        print(f"ğŸ”¥ ç«åŠ›: {len(GEMINI_KEYS)} Keys Ã— ä½µç™¼çŸ©é™£")
        print(f"ğŸ’° é ç®—: ${BUDGET_LIMIT} USD Ã— {len(GEMINI_KEYS)} = ${BUDGET_LIMIT * len(GEMINI_KEYS)} (à¸¿{BUDGET_LIMIT * len(GEMINI_KEYS) * EXCHANGE_RATE:.0f} THB)")
        print(f"ğŸ“‚ è¼¸å…¥: {INPUT_DIR}")
        print(f"ğŸ“‚ è¼¸å‡º: {OUTPUT_DIR}\n")
    
    def load_checkpoint(self):
        """åŠ è¼‰æª¢æŸ¥é»"""
        checkpoint_file = CHECKPOINT_DIR / 'master_checkpoint.json'
        if checkpoint_file.exists():
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.processed = data.get('processed_docs', 0)
                print(f"ğŸ“ å¾æª¢æŸ¥é»æ¢å¾©: å·²è™•ç† {self.processed} å€‹æ–‡æª”")
                return set(data.get('processed_hashes', []))
        return set()
    
    def save_checkpoint(self):
        """ä¿å­˜æª¢æŸ¥é»"""
        CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
        checkpoint_file = CHECKPOINT_DIR / 'master_checkpoint.json'
        
        data = {
            'processed_hashes': list(self.checkpoint),
            'processed_docs': self.processed,
            'costs': self.costs,
            'total_tokens': self.total_tokens,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)
    
    def get_file_hash(self, filepath):
        """è¨ˆç®—æ–‡ä»¶å“ˆå¸Œ"""
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def estimate_cost(self, input_text, output_text, key_idx):
        """ä¼°ç®—æˆæœ¬ï¼ˆGemini Proå®šåƒ¹ï¼‰"""
        input_tokens = len(input_text) // 4
        output_tokens = len(output_text) // 4
        
        # Gemini Pro: $0.0005/1K in, $0.0015/1K out
        cost = (input_tokens / 1000 * 0.0005) + (output_tokens / 1000 * 0.0015)
        
        self.costs[key_idx] += cost
        self.models[key_idx]['cost'] += cost
        self.total_tokens['input'] += input_tokens
        self.total_tokens['output'] += output_tokens
        
        return cost, input_tokens, output_tokens
    
    def distill_with_deep_thinking(self, doc_path, key_idx):
        """
        ğŸ§  Gemini 3.0 Pro æ·±åº¦æ€è€ƒ - 4éšæ®µè’¸é¤¾
        éšæ®µ1: ä½æº«å»é‡æª¢æ¸¬ (temp=0.3)
        éšæ®µ2: ç²¾è¯æå– (temp=0.3)
        éšæ®µ3: é‚è¼¯é‡æ§‹ (temp=0.5)
        éšæ®µ4: å‰µæ„æ¸²æŸ“ (temp=0.8)
        """
        try:
            with open(doc_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            doc_name = doc_path.name
            model_info = self.models[key_idx]
            model = model_info['model']
            
            # ğŸ” éšæ®µ1: ä½æº«å»é‡æª¢æ¸¬ (temperature=0.3)
            stage1_prompt = f"""ä½ æ˜¯æ•¸å­—è³‡ç”¢è’¸é¤¾å¤§å¸«ã€‚ç”¨ä½æº«ç²¾ç¢ºæ¨¡å¼åˆ†æé€™ä»½æ–‡æª”çš„ç¨ç‰¹åƒ¹å€¼ï¼š

æ–‡æª”: {doc_name}
å…§å®¹å‰3000å­—: {content[:3000]}

è«‹åš´æ ¼è©•ä¼°ï¼š
1. æ ¸å¿ƒåƒ¹å€¼æ˜¯ä»€éº¼ï¼Ÿï¼ˆæŠ€è¡“æ·±åº¦/å¯¦ç”¨æ€§/ç¨ç‰¹æ€§ï¼‰
2. æ˜¯å¦åŒ…å«ç½•è¦‹çŸ¥è­˜æˆ–ç¨ç‰¹è¦‹è§£ï¼Ÿ
3. èˆ‡å¸¸è¦‹æ–‡æª”çš„å·®ç•°åŒ–ç¨‹åº¦ï¼Ÿ
4. çµ¦å‡ºåƒ¹å€¼è©•åˆ†ï¼ˆ0-10åˆ†ï¼Œ8åˆ†ä»¥ä¸Šæ‰æ˜¯é»ƒé‡‘ï¼‰

è¼¸å‡ºæ ¼å¼ï¼š
è©•åˆ†: X/10
ç†ç”±: [ç°¡æ½”èªªæ˜]
"""
            
            response1 = model.generate_content(
                stage1_prompt,
                generation_config={'temperature': 0.3, 'top_p': 0.9}
            )
            value_analysis = response1.text
            
            # æå–è©•åˆ†
            score = 0
            for line in value_analysis.split('\n'):
                if 'è©•åˆ†' in line or 'score' in line.lower():
                    try:
                        score = float(''.join(c for c in line if c.isdigit() or c == '.'))
                        break
                    except:
                        pass
            
            # ä½æ–¼8åˆ†è·³éï¼ˆæé«˜é–€æª»ï¼‰
            if score < 8:
                print(f"â­ï¸  [Key-{key_idx+1}] è·³éä½åƒ¹å€¼æ–‡æª”: {doc_name} (è©•åˆ†: {score}/10)")
                return None
            
            print(f"âœ¨ [Key-{key_idx+1}] ç™¼ç¾æ•¸å­—é»ƒé‡‘: {doc_name} (è©•åˆ†: {score}/10)")
            
            # ğŸ“– éšæ®µ2: ç²¾è¯æå– (temperature=0.3)
            stage2_prompt = f"""æå–é€™ä»½æ•¸å­—é»ƒé‡‘çš„æ ¸å¿ƒç²¾è¯ï¼Œä¿æŒä½æº«ç²¾ç¢ºï¼š

å®Œæ•´å…§å®¹:
{content}

è«‹æå–ï¼š
- æ ¸å¿ƒæŠ€è¡“è¦é»ï¼ˆå…·é«”çš„ä»£ç¢¼/é…ç½®/å‘½ä»¤ï¼‰
- å¯¦ç”¨æŠ€å·§å’Œæœ€ä½³å¯¦è¸
- ç¨ç‰¹è¦‹è§£å’Œç¶“é©—ç¸½çµ
- é—œéµæ¦‚å¿µå’ŒåŸç†

è¦æ±‚ï¼šç²¾ç¢ºã€å»æ°´åˆ†ã€ä¿ç•™æ‰€æœ‰æŠ€è¡“ç´°ç¯€
"""
            
            response2 = model.generate_content(
                stage2_prompt,
                generation_config={'temperature': 0.3, 'top_p': 0.9}
            )
            essence = response2.text
            
            # ğŸ—ï¸ éšæ®µ3: é‚è¼¯é‡æ§‹ (temperature=0.5)
            stage3_prompt = f"""å°‡æå–çš„ç²¾è¯é€²è¡Œçµæ§‹åŒ–é‡æ§‹ï¼Œå¹³è¡¡é‚è¼¯èˆ‡å¯è®€æ€§ï¼š

æå–çš„ç²¾è¯:
{essence}

é‡æ§‹è¦æ±‚ï¼š
1. ä½¿ç”¨æ¸…æ™°çš„Markdownçµæ§‹
2. æŒ‰é‚è¼¯å±¤æ¬¡çµ„ç¹”ï¼ˆæ¦‚å¿µâ†’å¯¦ç¾â†’å¯¦è¸ï¼‰
3. ä¿ç•™æ‰€æœ‰æŠ€è¡“ç´°ç¯€
4. æ·»åŠ åˆé©çš„ä»£ç¢¼å¡Šå’Œç¤ºä¾‹
5. ä½¿ç”¨emojiå¢å¼·å¯è®€æ€§

è¼¸å‡ºå®Œæ•´çš„é‡æ§‹æ–‡æª”ã€‚
"""
            
            response3 = model.generate_content(
                stage3_prompt,
                generation_config={'temperature': 0.5, 'top_p': 0.95}
            )
            structured = response3.text
            
            # ğŸ¨ éšæ®µ4: å‰µæ„æ¸²æŸ“ (temperature=0.8)
            stage4_prompt = f"""åŸºæ–¼é‡æ§‹çš„çŸ¥è­˜ï¼Œé€²è¡Œå‰µæ„æ€§æ“´å±•å’Œå¯¦è¸å»ºè­°ï¼š

é‡æ§‹å¾Œçš„æ–‡æª”:
{structured}

å‰µæ„æ“´å±•ï¼š
1. å¯¦éš›æ‡‰ç”¨å ´æ™¯ï¼ˆ3-5å€‹å…·é«”æ¡ˆä¾‹ï¼‰
2. é€²éšå¯¦è¸å»ºè­°ï¼ˆå¦‚ä½•æ·±å…¥æŒæ¡ï¼‰
3. å¸¸è¦‹é™·é˜±èˆ‡è§£æ±ºæ–¹æ¡ˆ
4. èˆ‡å…¶ä»–æŠ€è¡“çš„å”åŒä½¿ç”¨
5. æœªä¾†ç™¼å±•æ–¹å‘

ä¿æŒå‰µæ„ä½†è¦å¯¦ç”¨ï¼Œä¸è¦ç©ºæ³›ã€‚
"""
            
            response4 = model.generate_content(
                stage4_prompt,
                generation_config={'temperature': 0.8, 'top_p': 0.95}
            )
            creative = response4.text
            
            # ğŸ’° è¨ˆç®—æˆæœ¬
            total_input = stage1_prompt + stage2_prompt + stage3_prompt + stage4_prompt
            total_output = value_analysis + essence + structured + creative
            cost, in_tok, out_tok = self.estimate_cost(total_input, total_output, key_idx)
            
            # ğŸ’¾ ä¿å­˜è’¸é¤¾çµæœ
            output_file = OUTPUT_DIR / f"distilled_{doc_name}"
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"# ğŸ¯ è«¸è‘›è’¸é¤¾å ±å‘Š: {doc_name}\n\n")
                f.write(f"> è’¸é¤¾æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"> åŸ·è¡Œå®¹å™¨: Key-{key_idx+1}\n")
                f.write(f"> åƒ¹å€¼è©•åˆ†: {score}/10 â­\n\n")
                f.write(f"---\n\n")
                f.write(f"## ğŸ” éšæ®µ1: ä½æº«å»é‡æª¢æ¸¬\n\n{value_analysis}\n\n")
                f.write(f"---\n\n")
                f.write(f"## ğŸ“– éšæ®µ2: ç²¾è¯æå–\n\n{essence}\n\n")
                f.write(f"---\n\n")
                f.write(f"## ğŸ—ï¸ éšæ®µ3: é‚è¼¯é‡æ§‹\n\n{structured}\n\n")
                f.write(f"---\n\n")
                f.write(f"## ğŸ¨ éšæ®µ4: å‰µæ„æ¸²æŸ“\n\n{creative}\n\n")
                f.write(f"---\n\n")
                f.write(f"## ğŸ“Š è’¸é¤¾å…ƒæ•¸æ“š\n\n")
                f.write(f"- ğŸ’° æˆæœ¬: ${cost:.4f} USD (à¸¿{cost * EXCHANGE_RATE:.2f} THB)\n")
                f.write(f"- ğŸ“ˆ Tokens: {in_tok:,} in / {out_tok:,} out\n")
                f.write(f"- ğŸ”¥ åŸ·è¡ŒKey: Key-{key_idx+1}\n")
                f.write(f"- â±ï¸ è’¸é¤¾æ™‚é–“: {datetime.now().isoformat()}\n")
            
            model_info['docs'] += 1
            
            return {
                'file': doc_name,
                'score': score,
                'cost': cost,
                'key': key_idx + 1
            }
            
        except Exception as e:
            print(f"âŒ [Key-{key_idx+1}] è’¸é¤¾å¤±æ•—: {doc_path.name} - {e}")
            return None
    
    def run_concurrent_distillation(self):
        """ä¸¦ç™¼è’¸é¤¾ä¸»æµç¨‹"""
        # ç²å–æ‰€æœ‰å¾…è™•ç†æ–‡æª”
        all_docs = list(INPUT_DIR.glob('**/*.md'))
        all_docs = [d for d in all_docs if self.get_file_hash(d) not in self.checkpoint]
        
        if not all_docs:
            print("ğŸ“­ æ²’æœ‰å¾…è™•ç†æ–‡æª”")
            return
        
        print(f"ğŸ“š æ‰¾åˆ° {len(all_docs)} å€‹å¾…è’¸é¤¾æ–‡æª”")
        print(f"ğŸ”¥ å•Ÿå‹• {MAX_WORKERS} å€‹ä½µç™¼å®¹å™¨...\n")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            futures = {}
            for idx, doc in enumerate(all_docs):
                key_idx = idx % len(GEMINI_KEYS)  # è¼ªè©¢åˆ†é…key
                
                # æª¢æŸ¥è©²keyæ˜¯å¦è¶…é ç®—
                if self.costs[key_idx] >= BUDGET_LIMIT:
                    print(f"âš ï¸  Key-{key_idx+1} å·²é”é ç®—ä¸Šé™ï¼Œè·³é")
                    continue
                
                future = executor.submit(self.distill_with_deep_thinking, doc, key_idx)
                futures[future] = (doc, key_idx)
            
            # é€²åº¦æ¢
            with tqdm(total=len(futures), desc="ğŸ¯ è’¸é¤¾é€²åº¦") as pbar:
                for future in as_completed(futures):
                    doc, key_idx = futures[future]
                    
                    try:
                        result = future.result()
                        if result:
                            results.append(result)
                            self.processed += 1
                            self.checkpoint.add(self.get_file_hash(doc))
                            self.save_checkpoint()
                            
                            pbar.set_postfix({
                                'processed': self.processed,
                                'cost': f"${sum(self.costs):.2f}",
                                'last_key': f"K{result['key']}"
                            })
                    except Exception as e:
                        print(f"âŒ ä»»å‹™å¤±æ•—: {doc.name} - {e}")
                    
                    pbar.update(1)
                    time.sleep(0.5)  # é˜²æ­¢APIé™æµ
        
        # æœ€çµ‚å ±å‘Š
        self.print_final_report(results)
    
    def print_final_report(self, results):
        """æ‰“å°æœ€çµ‚å ±å‘Š"""
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘            ğŸ¯ è«¸è‘›äº®Â·è³½åšè’¸é¤¾å¡” - æˆ°å ±                          â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        
        total_cost = sum(self.costs)
        
        print(f"ğŸ“Š è’¸é¤¾çµ±è¨ˆ:")
        print(f"   âœ… æˆåŠŸè’¸é¤¾: {len(results)} å€‹æ•¸å­—é»ƒé‡‘")
        print(f"   ğŸ“š ç¸½è™•ç†: {self.processed} æ–‡æª”")
        print(f"   ğŸ’° ç¸½æˆæœ¬: ${total_cost:.4f} USD (à¸¿{total_cost * EXCHANGE_RATE:.2f} THB)")
        print(f"   ğŸ“ˆ ç¸½Tokens: {self.total_tokens['input']:,} in / {self.total_tokens['output']:,} out\n")
        
        print("ğŸ”¥ å„Keyæˆ°ç¸¾:")
        for model in self.models:
            print(f"   Key-{model['id']}: {model['docs']} æ–‡æª” | ${model['cost']:.4f} USD (à¸¿{model['cost'] * EXCHANGE_RATE:.2f} THB)")
        
        # é«˜åˆ†æ–‡æª”
        high_scores = [r for r in results if r.get('score', 0) >= 9]
        if high_scores:
            print(f"\nğŸ’ æ¥µå“é»ƒé‡‘ (9åˆ†ä»¥ä¸Š): {len(high_scores)} å€‹")
            for r in high_scores[:5]:
                print(f"   â­ {r['file']} - {r['score']}/10")
        
        print(f"\nğŸ“ è’¸é¤¾çµæœ: {OUTPUT_DIR}")
        print("âœ… è«¸è‘›è’¸é¤¾å¡”ä»»å‹™å®Œæˆï¼\n")

if __name__ == '__main__':
    tower = ZhugeDistillationTower()
    tower.run_concurrent_distillation()
