---
distilled_by: grok-4-0709
mode: B
---
part: 7
---

## ğŸ”— çŸ¥è­˜åœ–è­œ
- [ç›¸é—œæ–‡æª”1: MoE åŸºç¤ç†è«–](2-knowledge-base/2.3-cyber-universe/moe-basics.md)
- [ç›¸é—œæ–‡æª”2: åšå¼ˆè«–åœ¨ AI ä¸­çš„æ‡‰ç”¨](2-knowledge-base/2.3-cyber-universe/game-theory-ai.md)
- [ç›¸é—œæ–‡æª”3: æœ¬åœ°æ¨ç†å¼•æ“æŒ‡å—](2-knowledge-base/2.3-cyber-universe/local-inference-guide.md)
- [ç›¸é—œæ–‡æª”4: Gemini API æœ€ä½³å¯¦è¸](2-knowledge-base/2.3-cyber-universe/gemini-api-best-practices.md)

vector_tags: ZHUGE LEGION, MoE System, Nash Equilibrium, Local Inference, Yi-6B, Gemini API, Mixture of Experts, Game Theory, Cost Optimization, AI Architecture, Red Teaming, Decision Engine