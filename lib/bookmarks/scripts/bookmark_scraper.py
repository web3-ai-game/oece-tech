#!/usr/bin/env python3
"""
ä¹¦ç­¾æ™ºèƒ½çˆ¬è™« v2.0
åŠŸèƒ½ï¼šæ·±åº¦2-3å±‚æŠ“å–é¡µé¢å…¨æ–‡
è¾“å…¥ï¼šcleaned_bookmarks.json
è¾“å‡ºï¼šscraped_content.json
"""

import json
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Set
import re
from datetime import datetime

class BookmarkScraper:
    def __init__(self, max_depth=3, max_pages_per_site=10):
        self.max_depth = max_depth
        self.max_pages_per_site = max_pages_per_site
        self.visited = set()
        self.session = None
        
    async def init_session(self):
        """åˆå§‹åŒ–HTTPä¼šè¯"""
        timeout = aiohttp.ClientTimeout(total=30)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        self.session = aiohttp.ClientSession(timeout=timeout, headers=headers)
    
    async def close_session(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            await self.session.close()
    
    def extract_text(self, html: str) -> str:
        """æå–é¡µé¢ä¸»è¦æ–‡æœ¬ï¼ˆå»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰å™ªéŸ³ï¼‰"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # åˆ é™¤æ— ç”¨æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe']):
            tag.decompose()
        
        # æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = soup.find(['main', 'article', 'div[role="main"]']) or soup.body
        if not main_content:
            return ""
        
        # æå–æ–‡æœ¬
        text = main_content.get_text(separator='\n', strip=True)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # é™åˆ¶é•¿åº¦ï¼ˆé¿å…è¶…å¤§é¡µé¢ï¼‰
        return text[:50000]  # æœ€å¤š50kå­—ç¬¦
    
    def extract_links(self, html: str, base_url: str) -> List[str]:
        """æå–é¡µé¢å†…é“¾æ¥"""
        soup = BeautifulSoup(html, 'html.parser')
        base_domain = urlparse(base_url).netloc
        
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            
            # è·³è¿‡é”šç‚¹å’Œé‚®ä»¶é“¾æ¥
            if href.startswith('#') or href.startswith('mailto:'):
                continue
            
            # è½¬ç»å¯¹è·¯å¾„
            abs_url = urljoin(base_url, href)
            
            # åªä¿ç•™åŒåŸŸåé“¾æ¥
            if urlparse(abs_url).netloc == base_domain:
                # è·³è¿‡PDFã€å›¾ç‰‡ç­‰
                if not re.search(r'\.(pdf|jpg|png|gif|zip|mp4)$', abs_url, re.I):
                    links.append(abs_url)
        
        return list(set(links))[:self.max_pages_per_site]
    
    async def fetch_page(self, url: str) -> tuple[str, str]:
        """æŠ“å–å•ä¸ªé¡µé¢"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    return html, None
                else:
                    return None, f"HTTP {response.status}"
        except asyncio.TimeoutError:
            return None, "Timeout"
        except Exception as e:
            return None, str(e)
    
    async def scrape_recursive(self, url: str, depth: int = 0) -> Dict:
        """é€’å½’æŠ“å–ï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰"""
        if depth > self.max_depth or url in self.visited:
            return None
        
        self.visited.add(url)
        print(f"  {'  ' * depth}[æ·±åº¦{depth}] æŠ“å–: {url}")
        
        # æŠ“å–å½“å‰é¡µ
        html, error = await self.fetch_page(url)
        if error:
            print(f"  {'  ' * depth}âœ— å¤±è´¥: {error}")
            return None
        
        # æå–æ–‡æœ¬
        text = self.extract_text(html)
        
        result = {
            'url': url,
            'depth': depth,
            'text': text,
            'text_length': len(text),
            'children': []
        }
        
        # å¦‚æœæœªè¾¾æœ€å¤§æ·±åº¦ï¼Œç»§ç»­æŠ“å–å­é¡µé¢
        if depth < self.max_depth:
            links = self.extract_links(html, url)
            print(f"  {'  ' * depth}â”œâ”€ å‘ç° {len(links)} ä¸ªå­é“¾æ¥")
            
            # å¹¶å‘æŠ“å–å­é¡µé¢ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
            tasks = [self.scrape_recursive(link, depth + 1) for link in links[:5]]
            children = await asyncio.gather(*tasks)
            result['children'] = [c for c in children if c]
        
        print(f"  {'  ' * depth}âœ“ å®Œæˆ: {len(text)} å­—ç¬¦")
        return result
    
    async def scrape_bookmark(self, bookmark: Dict) -> Dict:
        """æŠ“å–å•ä¸ªä¹¦ç­¾çš„å†…å®¹"""
        url = bookmark['url']
        print(f"\nğŸ“„ å¤„ç†: {bookmark['title']}")
        
        try:
            content = await self.scrape_recursive(url, depth=0)
            
            if content:
                # ç»Ÿè®¡æ€»å­—ç¬¦æ•°
                total_chars = self._count_chars(content)
                print(f"âœ“ å®Œæˆ: æ€»è®¡ {total_chars} å­—ç¬¦")
                
                return {
                    **bookmark,
                    'scraped_at': datetime.utcnow().isoformat(),
                    'content': content,
                    'total_chars': total_chars
                }
            else:
                print(f"âœ— æŠ“å–å¤±è´¥")
                return {
                    **bookmark,
                    'scraped_at': datetime.utcnow().isoformat(),
                    'content': None,
                    'error': 'Failed to scrape'
                }
        
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return {
                **bookmark,
                'scraped_at': datetime.utcnow().isoformat(),
                'content': None,
                'error': str(e)
            }
    
    def _count_chars(self, node: Dict) -> int:
        """é€’å½’ç»Ÿè®¡å­—ç¬¦æ•°"""
        if not node:
            return 0
        total = node.get('text_length', 0)
        for child in node.get('children', []):
            total += self._count_chars(child)
        return total
    
    async def scrape_all(self, bookmarks: List[Dict], limit: int = None):
        """æ‰¹é‡æŠ“å–æ‰€æœ‰ä¹¦ç­¾"""
        if limit:
            bookmarks = bookmarks[:limit]
        
        print(f"ğŸš€ å¼€å§‹æŠ“å– {len(bookmarks)} ä¸ªä¹¦ç­¾...")
        print(f"   æœ€å¤§æ·±åº¦: {self.max_depth}")
        print(f"   æ¯ç«™æœ€å¤š: {self.max_pages_per_site} é¡µ\n")
        
        await self.init_session()
        
        results = []
        for i, bm in enumerate(bookmarks, 1):
            print(f"\n[{i}/{len(bookmarks)}] ", end="")
            result = await self.scrape_bookmark(bm)
            results.append(result)
            
            # ç¤¼è²Œå»¶è¿Ÿ
            await asyncio.sleep(1)
        
        await self.close_session()
        
        # ç»Ÿè®¡
        success = sum(1 for r in results if r.get('content'))
        total_chars = sum(r.get('total_chars', 0) for r in results)
        
        print(f"\n\nâœ… æŠ“å–å®Œæˆ:")
        print(f"   æˆåŠŸ: {success}/{len(results)}")
        print(f"   æ€»å­—ç¬¦: {total_chars:,}")
        print(f"   å¹³å‡: {total_chars//len(results):,} å­—ç¬¦/ç«™")
        
        return results


async def main():
    # åŠ è½½æ¸…æ´—åçš„ä¹¦ç­¾
    with open('cleaned_bookmarks.json', 'r', encoding='utf-8') as f:
        bookmarks = json.load(f)
    
    print(f"ğŸ“š åŠ è½½ {len(bookmarks)} æ¡ä¹¦ç­¾")
    
    # åªæŠ“å–5æ˜Ÿå’Œ4æ˜Ÿèµ„æºï¼ˆèŠ‚çœæ—¶é—´ï¼‰
    high_value = [
        bm for bm in bookmarks 
        if bm.get('importance', 0) >= 4
    ]
    
    print(f"ğŸ¯ ç­›é€‰å‡º {len(high_value)} æ¡é«˜ä»·å€¼ä¹¦ç­¾ï¼ˆ4-5æ˜Ÿï¼‰\n")
    
    # å¼€å§‹æŠ“å–ï¼ˆå¯é€‰ï¼šé™åˆ¶æ•°é‡æµ‹è¯•ï¼‰
    scraper = BookmarkScraper(max_depth=2, max_pages_per_site=5)
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªæŠ“5ä¸ª
    # results = await scraper.scrape_all(high_value[:5])
    
    # å®Œæ•´æ¨¡å¼
    results = await scraper.scrape_all(high_value)
    
    # ä¿å­˜ç»“æœ
    output_file = 'scraped_content.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        'total_bookmarks': len(bookmarks),
        'high_value_bookmarks': len(high_value),
        'scraped_successfully': sum(1 for r in results if r.get('content')),
        'total_characters': sum(r.get('total_chars', 0) for r in results),
        'average_chars_per_site': sum(r.get('total_chars', 0) for r in results) // len(results),
        'timestamp': datetime.utcnow().isoformat()
    }
    
    with open('scraping_stats.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: scraping_stats.json")


if __name__ == '__main__':
    asyncio.run(main())
